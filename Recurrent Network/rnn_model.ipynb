{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13e21cb0",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c366fe53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217ce51c",
   "metadata": {},
   "source": [
    "Set Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce8f897f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50827543",
   "metadata": {},
   "source": [
    "Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca0f6665",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 28\n",
    "sequence_length = 28\n",
    "num_layers = 2\n",
    "hidden_size = 256\n",
    "num_classes = 10\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "num_epochs = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a88ec2",
   "metadata": {},
   "source": [
    "Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc708885",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.MNIST(root='./datatset', train=True, transform=transforms.ToTensor(), download=True)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True,)\n",
    "test_dataset = datasets.MNIST(root='./datatset', train=False, transform=transforms.ToTensor(), download=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ec4696",
   "metadata": {},
   "source": [
    "Create Fully Connected Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e5b9b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(in_features=hidden_size*sequence_length, out_features=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "\n",
    "        #Forward Propagation\n",
    "        out, _ = self.rnn(x,h0)\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3594783e",
   "metadata": {},
   "source": [
    "Initialize the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b43973fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN(input_size=input_size,num_classes=num_classes, hidden_size=hidden_size, num_layers=num_layers).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d21ae9c",
   "metadata": {},
   "source": [
    "Loss and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5dfab2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e249edbc",
   "metadata": {},
   "source": [
    "Train the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "875b618d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2], Step [1/938], Loss: 2.3064\n",
      "Epoch [1/2], Step [2/938], Loss: 2.3869\n",
      "Epoch [1/2], Step [3/938], Loss: 2.2599\n",
      "Epoch [1/2], Step [4/938], Loss: 2.1011\n",
      "Epoch [1/2], Step [5/938], Loss: 1.9615\n",
      "Epoch [1/2], Step [6/938], Loss: 1.6223\n",
      "Epoch [1/2], Step [7/938], Loss: 1.5092\n",
      "Epoch [1/2], Step [8/938], Loss: 1.3050\n",
      "Epoch [1/2], Step [9/938], Loss: 1.2690\n",
      "Epoch [1/2], Step [10/938], Loss: 1.0497\n",
      "Epoch [1/2], Step [11/938], Loss: 0.7948\n",
      "Epoch [1/2], Step [12/938], Loss: 0.8340\n",
      "Epoch [1/2], Step [13/938], Loss: 1.0050\n",
      "Epoch [1/2], Step [14/938], Loss: 1.0238\n",
      "Epoch [1/2], Step [15/938], Loss: 0.6996\n",
      "Epoch [1/2], Step [16/938], Loss: 0.6797\n",
      "Epoch [1/2], Step [17/938], Loss: 0.6685\n",
      "Epoch [1/2], Step [18/938], Loss: 0.5645\n",
      "Epoch [1/2], Step [19/938], Loss: 0.6442\n",
      "Epoch [1/2], Step [20/938], Loss: 0.8347\n",
      "Epoch [1/2], Step [21/938], Loss: 0.7379\n",
      "Epoch [1/2], Step [22/938], Loss: 0.7113\n",
      "Epoch [1/2], Step [23/938], Loss: 0.8042\n",
      "Epoch [1/2], Step [24/938], Loss: 0.3100\n",
      "Epoch [1/2], Step [25/938], Loss: 0.7196\n",
      "Epoch [1/2], Step [26/938], Loss: 0.4424\n",
      "Epoch [1/2], Step [27/938], Loss: 0.4148\n",
      "Epoch [1/2], Step [28/938], Loss: 0.3865\n",
      "Epoch [1/2], Step [29/938], Loss: 0.6479\n",
      "Epoch [1/2], Step [30/938], Loss: 0.6258\n",
      "Epoch [1/2], Step [31/938], Loss: 0.5970\n",
      "Epoch [1/2], Step [32/938], Loss: 0.6665\n",
      "Epoch [1/2], Step [33/938], Loss: 0.5044\n",
      "Epoch [1/2], Step [34/938], Loss: 0.6354\n",
      "Epoch [1/2], Step [35/938], Loss: 0.6656\n",
      "Epoch [1/2], Step [36/938], Loss: 0.5047\n",
      "Epoch [1/2], Step [37/938], Loss: 0.5152\n",
      "Epoch [1/2], Step [38/938], Loss: 0.4683\n",
      "Epoch [1/2], Step [39/938], Loss: 0.5421\n",
      "Epoch [1/2], Step [40/938], Loss: 0.4161\n",
      "Epoch [1/2], Step [41/938], Loss: 0.6493\n",
      "Epoch [1/2], Step [42/938], Loss: 0.4734\n",
      "Epoch [1/2], Step [43/938], Loss: 0.6279\n",
      "Epoch [1/2], Step [44/938], Loss: 0.7276\n",
      "Epoch [1/2], Step [45/938], Loss: 0.6731\n",
      "Epoch [1/2], Step [46/938], Loss: 0.3994\n",
      "Epoch [1/2], Step [47/938], Loss: 0.4840\n",
      "Epoch [1/2], Step [48/938], Loss: 0.5583\n",
      "Epoch [1/2], Step [49/938], Loss: 0.4623\n",
      "Epoch [1/2], Step [50/938], Loss: 0.6419\n",
      "Epoch [1/2], Step [51/938], Loss: 0.4797\n",
      "Epoch [1/2], Step [52/938], Loss: 0.2461\n",
      "Epoch [1/2], Step [53/938], Loss: 0.3751\n",
      "Epoch [1/2], Step [54/938], Loss: 0.4672\n",
      "Epoch [1/2], Step [55/938], Loss: 0.3315\n",
      "Epoch [1/2], Step [56/938], Loss: 0.3987\n",
      "Epoch [1/2], Step [57/938], Loss: 0.4635\n",
      "Epoch [1/2], Step [58/938], Loss: 0.5194\n",
      "Epoch [1/2], Step [59/938], Loss: 0.6181\n",
      "Epoch [1/2], Step [60/938], Loss: 0.4294\n",
      "Epoch [1/2], Step [61/938], Loss: 0.4871\n",
      "Epoch [1/2], Step [62/938], Loss: 0.4468\n",
      "Epoch [1/2], Step [63/938], Loss: 0.7474\n",
      "Epoch [1/2], Step [64/938], Loss: 0.4146\n",
      "Epoch [1/2], Step [65/938], Loss: 0.4641\n",
      "Epoch [1/2], Step [66/938], Loss: 0.6804\n",
      "Epoch [1/2], Step [67/938], Loss: 0.6077\n",
      "Epoch [1/2], Step [68/938], Loss: 0.4961\n",
      "Epoch [1/2], Step [69/938], Loss: 0.4882\n",
      "Epoch [1/2], Step [70/938], Loss: 0.3167\n",
      "Epoch [1/2], Step [71/938], Loss: 0.4123\n",
      "Epoch [1/2], Step [72/938], Loss: 0.4000\n",
      "Epoch [1/2], Step [73/938], Loss: 0.2952\n",
      "Epoch [1/2], Step [74/938], Loss: 0.5779\n",
      "Epoch [1/2], Step [75/938], Loss: 0.5733\n",
      "Epoch [1/2], Step [76/938], Loss: 0.4496\n",
      "Epoch [1/2], Step [77/938], Loss: 0.3589\n",
      "Epoch [1/2], Step [78/938], Loss: 0.2848\n",
      "Epoch [1/2], Step [79/938], Loss: 0.3540\n",
      "Epoch [1/2], Step [80/938], Loss: 0.4248\n",
      "Epoch [1/2], Step [81/938], Loss: 0.2341\n",
      "Epoch [1/2], Step [82/938], Loss: 0.6029\n",
      "Epoch [1/2], Step [83/938], Loss: 0.4205\n",
      "Epoch [1/2], Step [84/938], Loss: 0.5882\n",
      "Epoch [1/2], Step [85/938], Loss: 0.6589\n",
      "Epoch [1/2], Step [86/938], Loss: 0.2276\n",
      "Epoch [1/2], Step [87/938], Loss: 0.3768\n",
      "Epoch [1/2], Step [88/938], Loss: 0.3115\n",
      "Epoch [1/2], Step [89/938], Loss: 0.4053\n",
      "Epoch [1/2], Step [90/938], Loss: 0.3176\n",
      "Epoch [1/2], Step [91/938], Loss: 0.2920\n",
      "Epoch [1/2], Step [92/938], Loss: 0.5135\n",
      "Epoch [1/2], Step [93/938], Loss: 0.3732\n",
      "Epoch [1/2], Step [94/938], Loss: 0.4887\n",
      "Epoch [1/2], Step [95/938], Loss: 0.4595\n",
      "Epoch [1/2], Step [96/938], Loss: 0.4869\n",
      "Epoch [1/2], Step [97/938], Loss: 0.6418\n",
      "Epoch [1/2], Step [98/938], Loss: 0.4799\n",
      "Epoch [1/2], Step [99/938], Loss: 0.2695\n",
      "Epoch [1/2], Step [100/938], Loss: 0.3322\n",
      "Epoch [1/2], Step [101/938], Loss: 0.5434\n",
      "Epoch [1/2], Step [102/938], Loss: 0.2180\n",
      "Epoch [1/2], Step [103/938], Loss: 0.6306\n",
      "Epoch [1/2], Step [104/938], Loss: 0.4033\n",
      "Epoch [1/2], Step [105/938], Loss: 0.3444\n",
      "Epoch [1/2], Step [106/938], Loss: 0.5379\n",
      "Epoch [1/2], Step [107/938], Loss: 0.4212\n",
      "Epoch [1/2], Step [108/938], Loss: 0.3164\n",
      "Epoch [1/2], Step [109/938], Loss: 0.4714\n",
      "Epoch [1/2], Step [110/938], Loss: 0.3236\n",
      "Epoch [1/2], Step [111/938], Loss: 0.4376\n",
      "Epoch [1/2], Step [112/938], Loss: 0.3721\n",
      "Epoch [1/2], Step [113/938], Loss: 0.2935\n",
      "Epoch [1/2], Step [114/938], Loss: 0.1501\n",
      "Epoch [1/2], Step [115/938], Loss: 0.6155\n",
      "Epoch [1/2], Step [116/938], Loss: 0.3142\n",
      "Epoch [1/2], Step [117/938], Loss: 0.2426\n",
      "Epoch [1/2], Step [118/938], Loss: 0.3261\n",
      "Epoch [1/2], Step [119/938], Loss: 0.3936\n",
      "Epoch [1/2], Step [120/938], Loss: 0.3552\n",
      "Epoch [1/2], Step [121/938], Loss: 0.4962\n",
      "Epoch [1/2], Step [122/938], Loss: 0.2579\n",
      "Epoch [1/2], Step [123/938], Loss: 0.3442\n",
      "Epoch [1/2], Step [124/938], Loss: 0.2037\n",
      "Epoch [1/2], Step [125/938], Loss: 0.2986\n",
      "Epoch [1/2], Step [126/938], Loss: 0.4483\n",
      "Epoch [1/2], Step [127/938], Loss: 0.3321\n",
      "Epoch [1/2], Step [128/938], Loss: 0.4413\n",
      "Epoch [1/2], Step [129/938], Loss: 0.3422\n",
      "Epoch [1/2], Step [130/938], Loss: 0.3458\n",
      "Epoch [1/2], Step [131/938], Loss: 0.3826\n",
      "Epoch [1/2], Step [132/938], Loss: 0.1204\n",
      "Epoch [1/2], Step [133/938], Loss: 0.4109\n",
      "Epoch [1/2], Step [134/938], Loss: 0.3687\n",
      "Epoch [1/2], Step [135/938], Loss: 0.4034\n",
      "Epoch [1/2], Step [136/938], Loss: 0.4301\n",
      "Epoch [1/2], Step [137/938], Loss: 0.2394\n",
      "Epoch [1/2], Step [138/938], Loss: 0.5736\n",
      "Epoch [1/2], Step [139/938], Loss: 0.6037\n",
      "Epoch [1/2], Step [140/938], Loss: 0.4797\n",
      "Epoch [1/2], Step [141/938], Loss: 0.4005\n",
      "Epoch [1/2], Step [142/938], Loss: 0.6273\n",
      "Epoch [1/2], Step [143/938], Loss: 0.2938\n",
      "Epoch [1/2], Step [144/938], Loss: 0.2811\n",
      "Epoch [1/2], Step [145/938], Loss: 0.6500\n",
      "Epoch [1/2], Step [146/938], Loss: 0.2730\n",
      "Epoch [1/2], Step [147/938], Loss: 0.3279\n",
      "Epoch [1/2], Step [148/938], Loss: 0.7590\n",
      "Epoch [1/2], Step [149/938], Loss: 0.2267\n",
      "Epoch [1/2], Step [150/938], Loss: 0.3516\n",
      "Epoch [1/2], Step [151/938], Loss: 0.3959\n",
      "Epoch [1/2], Step [152/938], Loss: 0.4028\n",
      "Epoch [1/2], Step [153/938], Loss: 0.3529\n",
      "Epoch [1/2], Step [154/938], Loss: 0.6220\n",
      "Epoch [1/2], Step [155/938], Loss: 0.3198\n",
      "Epoch [1/2], Step [156/938], Loss: 0.3305\n",
      "Epoch [1/2], Step [157/938], Loss: 0.4160\n",
      "Epoch [1/2], Step [158/938], Loss: 0.4953\n",
      "Epoch [1/2], Step [159/938], Loss: 0.5077\n",
      "Epoch [1/2], Step [160/938], Loss: 0.3247\n",
      "Epoch [1/2], Step [161/938], Loss: 0.5286\n",
      "Epoch [1/2], Step [162/938], Loss: 0.4749\n",
      "Epoch [1/2], Step [163/938], Loss: 0.2441\n",
      "Epoch [1/2], Step [164/938], Loss: 0.3773\n",
      "Epoch [1/2], Step [165/938], Loss: 0.1007\n",
      "Epoch [1/2], Step [166/938], Loss: 0.2928\n",
      "Epoch [1/2], Step [167/938], Loss: 0.3556\n",
      "Epoch [1/2], Step [168/938], Loss: 0.3786\n",
      "Epoch [1/2], Step [169/938], Loss: 0.4920\n",
      "Epoch [1/2], Step [170/938], Loss: 0.4863\n",
      "Epoch [1/2], Step [171/938], Loss: 0.6391\n",
      "Epoch [1/2], Step [172/938], Loss: 0.4299\n",
      "Epoch [1/2], Step [173/938], Loss: 0.3699\n",
      "Epoch [1/2], Step [174/938], Loss: 0.2522\n",
      "Epoch [1/2], Step [175/938], Loss: 0.8091\n",
      "Epoch [1/2], Step [176/938], Loss: 0.4445\n",
      "Epoch [1/2], Step [177/938], Loss: 0.2205\n",
      "Epoch [1/2], Step [178/938], Loss: 0.3641\n",
      "Epoch [1/2], Step [179/938], Loss: 0.5579\n",
      "Epoch [1/2], Step [180/938], Loss: 0.2690\n",
      "Epoch [1/2], Step [181/938], Loss: 0.2587\n",
      "Epoch [1/2], Step [182/938], Loss: 0.2249\n",
      "Epoch [1/2], Step [183/938], Loss: 0.3222\n",
      "Epoch [1/2], Step [184/938], Loss: 0.4332\n",
      "Epoch [1/2], Step [185/938], Loss: 0.4952\n",
      "Epoch [1/2], Step [186/938], Loss: 0.2410\n",
      "Epoch [1/2], Step [187/938], Loss: 0.2802\n",
      "Epoch [1/2], Step [188/938], Loss: 0.2287\n",
      "Epoch [1/2], Step [189/938], Loss: 0.3126\n",
      "Epoch [1/2], Step [190/938], Loss: 0.2494\n",
      "Epoch [1/2], Step [191/938], Loss: 0.2735\n",
      "Epoch [1/2], Step [192/938], Loss: 0.2311\n",
      "Epoch [1/2], Step [193/938], Loss: 0.4238\n",
      "Epoch [1/2], Step [194/938], Loss: 0.4081\n",
      "Epoch [1/2], Step [195/938], Loss: 0.2625\n",
      "Epoch [1/2], Step [196/938], Loss: 0.1599\n",
      "Epoch [1/2], Step [197/938], Loss: 0.4573\n",
      "Epoch [1/2], Step [198/938], Loss: 0.2522\n",
      "Epoch [1/2], Step [199/938], Loss: 0.1836\n",
      "Epoch [1/2], Step [200/938], Loss: 0.3088\n",
      "Epoch [1/2], Step [201/938], Loss: 0.4264\n",
      "Epoch [1/2], Step [202/938], Loss: 0.3405\n",
      "Epoch [1/2], Step [203/938], Loss: 0.2335\n",
      "Epoch [1/2], Step [204/938], Loss: 0.1151\n",
      "Epoch [1/2], Step [205/938], Loss: 0.4661\n",
      "Epoch [1/2], Step [206/938], Loss: 0.2862\n",
      "Epoch [1/2], Step [207/938], Loss: 0.2635\n",
      "Epoch [1/2], Step [208/938], Loss: 0.2126\n",
      "Epoch [1/2], Step [209/938], Loss: 0.2448\n",
      "Epoch [1/2], Step [210/938], Loss: 0.5628\n",
      "Epoch [1/2], Step [211/938], Loss: 0.2892\n",
      "Epoch [1/2], Step [212/938], Loss: 0.4463\n",
      "Epoch [1/2], Step [213/938], Loss: 0.4786\n",
      "Epoch [1/2], Step [214/938], Loss: 0.2688\n",
      "Epoch [1/2], Step [215/938], Loss: 0.2118\n",
      "Epoch [1/2], Step [216/938], Loss: 0.3298\n",
      "Epoch [1/2], Step [217/938], Loss: 0.5284\n",
      "Epoch [1/2], Step [218/938], Loss: 0.4636\n",
      "Epoch [1/2], Step [219/938], Loss: 0.4036\n",
      "Epoch [1/2], Step [220/938], Loss: 0.4176\n",
      "Epoch [1/2], Step [221/938], Loss: 0.4309\n",
      "Epoch [1/2], Step [222/938], Loss: 0.3211\n",
      "Epoch [1/2], Step [223/938], Loss: 0.2286\n",
      "Epoch [1/2], Step [224/938], Loss: 0.5264\n",
      "Epoch [1/2], Step [225/938], Loss: 0.2345\n",
      "Epoch [1/2], Step [226/938], Loss: 0.2608\n",
      "Epoch [1/2], Step [227/938], Loss: 0.4853\n",
      "Epoch [1/2], Step [228/938], Loss: 0.4581\n",
      "Epoch [1/2], Step [229/938], Loss: 0.3664\n",
      "Epoch [1/2], Step [230/938], Loss: 0.2606\n",
      "Epoch [1/2], Step [231/938], Loss: 0.2163\n",
      "Epoch [1/2], Step [232/938], Loss: 0.5193\n",
      "Epoch [1/2], Step [233/938], Loss: 0.4685\n",
      "Epoch [1/2], Step [234/938], Loss: 0.3639\n",
      "Epoch [1/2], Step [235/938], Loss: 0.3323\n",
      "Epoch [1/2], Step [236/938], Loss: 0.4732\n",
      "Epoch [1/2], Step [237/938], Loss: 0.1843\n",
      "Epoch [1/2], Step [238/938], Loss: 0.4098\n",
      "Epoch [1/2], Step [239/938], Loss: 0.4723\n",
      "Epoch [1/2], Step [240/938], Loss: 0.4885\n",
      "Epoch [1/2], Step [241/938], Loss: 0.1356\n",
      "Epoch [1/2], Step [242/938], Loss: 0.2534\n",
      "Epoch [1/2], Step [243/938], Loss: 0.5687\n",
      "Epoch [1/2], Step [244/938], Loss: 0.4741\n",
      "Epoch [1/2], Step [245/938], Loss: 0.2185\n",
      "Epoch [1/2], Step [246/938], Loss: 0.2011\n",
      "Epoch [1/2], Step [247/938], Loss: 0.1728\n",
      "Epoch [1/2], Step [248/938], Loss: 0.2183\n",
      "Epoch [1/2], Step [249/938], Loss: 0.2940\n",
      "Epoch [1/2], Step [250/938], Loss: 0.5705\n",
      "Epoch [1/2], Step [251/938], Loss: 0.3048\n",
      "Epoch [1/2], Step [252/938], Loss: 0.1519\n",
      "Epoch [1/2], Step [253/938], Loss: 0.1154\n",
      "Epoch [1/2], Step [254/938], Loss: 0.4326\n",
      "Epoch [1/2], Step [255/938], Loss: 0.2174\n",
      "Epoch [1/2], Step [256/938], Loss: 0.3805\n",
      "Epoch [1/2], Step [257/938], Loss: 0.5818\n",
      "Epoch [1/2], Step [258/938], Loss: 0.2822\n",
      "Epoch [1/2], Step [259/938], Loss: 0.1874\n",
      "Epoch [1/2], Step [260/938], Loss: 0.1964\n",
      "Epoch [1/2], Step [261/938], Loss: 0.2713\n",
      "Epoch [1/2], Step [262/938], Loss: 0.3604\n",
      "Epoch [1/2], Step [263/938], Loss: 0.3118\n",
      "Epoch [1/2], Step [264/938], Loss: 0.3292\n",
      "Epoch [1/2], Step [265/938], Loss: 0.2073\n",
      "Epoch [1/2], Step [266/938], Loss: 0.4584\n",
      "Epoch [1/2], Step [267/938], Loss: 0.3729\n",
      "Epoch [1/2], Step [268/938], Loss: 0.2280\n",
      "Epoch [1/2], Step [269/938], Loss: 0.2414\n",
      "Epoch [1/2], Step [270/938], Loss: 0.1901\n",
      "Epoch [1/2], Step [271/938], Loss: 0.1861\n",
      "Epoch [1/2], Step [272/938], Loss: 0.1177\n",
      "Epoch [1/2], Step [273/938], Loss: 0.2127\n",
      "Epoch [1/2], Step [274/938], Loss: 0.2440\n",
      "Epoch [1/2], Step [275/938], Loss: 0.3656\n",
      "Epoch [1/2], Step [276/938], Loss: 0.2034\n",
      "Epoch [1/2], Step [277/938], Loss: 0.3127\n",
      "Epoch [1/2], Step [278/938], Loss: 0.1526\n",
      "Epoch [1/2], Step [279/938], Loss: 0.2647\n",
      "Epoch [1/2], Step [280/938], Loss: 0.3618\n",
      "Epoch [1/2], Step [281/938], Loss: 0.4094\n",
      "Epoch [1/2], Step [282/938], Loss: 0.3978\n",
      "Epoch [1/2], Step [283/938], Loss: 0.3165\n",
      "Epoch [1/2], Step [284/938], Loss: 0.5088\n",
      "Epoch [1/2], Step [285/938], Loss: 0.1762\n",
      "Epoch [1/2], Step [286/938], Loss: 0.3291\n",
      "Epoch [1/2], Step [287/938], Loss: 0.2464\n",
      "Epoch [1/2], Step [288/938], Loss: 0.1786\n",
      "Epoch [1/2], Step [289/938], Loss: 0.3067\n",
      "Epoch [1/2], Step [290/938], Loss: 0.1744\n",
      "Epoch [1/2], Step [291/938], Loss: 0.4382\n",
      "Epoch [1/2], Step [292/938], Loss: 0.1200\n",
      "Epoch [1/2], Step [293/938], Loss: 0.0891\n",
      "Epoch [1/2], Step [294/938], Loss: 0.3075\n",
      "Epoch [1/2], Step [295/938], Loss: 0.2673\n",
      "Epoch [1/2], Step [296/938], Loss: 0.3054\n",
      "Epoch [1/2], Step [297/938], Loss: 0.4155\n",
      "Epoch [1/2], Step [298/938], Loss: 0.2689\n",
      "Epoch [1/2], Step [299/938], Loss: 0.2074\n",
      "Epoch [1/2], Step [300/938], Loss: 0.1875\n",
      "Epoch [1/2], Step [301/938], Loss: 0.3456\n",
      "Epoch [1/2], Step [302/938], Loss: 0.2110\n",
      "Epoch [1/2], Step [303/938], Loss: 0.2055\n",
      "Epoch [1/2], Step [304/938], Loss: 0.1080\n",
      "Epoch [1/2], Step [305/938], Loss: 0.0747\n",
      "Epoch [1/2], Step [306/938], Loss: 0.2910\n",
      "Epoch [1/2], Step [307/938], Loss: 0.3294\n",
      "Epoch [1/2], Step [308/938], Loss: 0.5080\n",
      "Epoch [1/2], Step [309/938], Loss: 0.3891\n",
      "Epoch [1/2], Step [310/938], Loss: 0.1459\n",
      "Epoch [1/2], Step [311/938], Loss: 0.1966\n",
      "Epoch [1/2], Step [312/938], Loss: 0.3634\n",
      "Epoch [1/2], Step [313/938], Loss: 0.2752\n",
      "Epoch [1/2], Step [314/938], Loss: 0.2311\n",
      "Epoch [1/2], Step [315/938], Loss: 0.3332\n",
      "Epoch [1/2], Step [316/938], Loss: 0.4962\n",
      "Epoch [1/2], Step [317/938], Loss: 0.2101\n",
      "Epoch [1/2], Step [318/938], Loss: 0.2926\n",
      "Epoch [1/2], Step [319/938], Loss: 0.3533\n",
      "Epoch [1/2], Step [320/938], Loss: 0.1399\n",
      "Epoch [1/2], Step [321/938], Loss: 0.4936\n",
      "Epoch [1/2], Step [322/938], Loss: 0.1471\n",
      "Epoch [1/2], Step [323/938], Loss: 0.2408\n",
      "Epoch [1/2], Step [324/938], Loss: 0.3015\n",
      "Epoch [1/2], Step [325/938], Loss: 0.2734\n",
      "Epoch [1/2], Step [326/938], Loss: 0.2407\n",
      "Epoch [1/2], Step [327/938], Loss: 0.3927\n",
      "Epoch [1/2], Step [328/938], Loss: 0.2669\n",
      "Epoch [1/2], Step [329/938], Loss: 0.1823\n",
      "Epoch [1/2], Step [330/938], Loss: 0.2574\n",
      "Epoch [1/2], Step [331/938], Loss: 0.3897\n",
      "Epoch [1/2], Step [332/938], Loss: 0.2203\n",
      "Epoch [1/2], Step [333/938], Loss: 0.2835\n",
      "Epoch [1/2], Step [334/938], Loss: 0.2235\n",
      "Epoch [1/2], Step [335/938], Loss: 0.2388\n",
      "Epoch [1/2], Step [336/938], Loss: 0.2404\n",
      "Epoch [1/2], Step [337/938], Loss: 0.2202\n",
      "Epoch [1/2], Step [338/938], Loss: 0.2361\n",
      "Epoch [1/2], Step [339/938], Loss: 0.1045\n",
      "Epoch [1/2], Step [340/938], Loss: 0.1154\n",
      "Epoch [1/2], Step [341/938], Loss: 0.1965\n",
      "Epoch [1/2], Step [342/938], Loss: 0.4731\n",
      "Epoch [1/2], Step [343/938], Loss: 0.3737\n",
      "Epoch [1/2], Step [344/938], Loss: 0.2431\n",
      "Epoch [1/2], Step [345/938], Loss: 0.4116\n",
      "Epoch [1/2], Step [346/938], Loss: 0.1547\n",
      "Epoch [1/2], Step [347/938], Loss: 0.1143\n",
      "Epoch [1/2], Step [348/938], Loss: 0.1219\n",
      "Epoch [1/2], Step [349/938], Loss: 0.3442\n",
      "Epoch [1/2], Step [350/938], Loss: 0.2490\n",
      "Epoch [1/2], Step [351/938], Loss: 0.2600\n",
      "Epoch [1/2], Step [352/938], Loss: 0.3177\n",
      "Epoch [1/2], Step [353/938], Loss: 0.2233\n",
      "Epoch [1/2], Step [354/938], Loss: 0.1331\n",
      "Epoch [1/2], Step [355/938], Loss: 0.5510\n",
      "Epoch [1/2], Step [356/938], Loss: 0.2185\n",
      "Epoch [1/2], Step [357/938], Loss: 0.2325\n",
      "Epoch [1/2], Step [358/938], Loss: 0.3770\n",
      "Epoch [1/2], Step [359/938], Loss: 0.1059\n",
      "Epoch [1/2], Step [360/938], Loss: 0.6046\n",
      "Epoch [1/2], Step [361/938], Loss: 0.4629\n",
      "Epoch [1/2], Step [362/938], Loss: 0.4295\n",
      "Epoch [1/2], Step [363/938], Loss: 0.4310\n",
      "Epoch [1/2], Step [364/938], Loss: 0.3735\n",
      "Epoch [1/2], Step [365/938], Loss: 0.1133\n",
      "Epoch [1/2], Step [366/938], Loss: 0.3934\n",
      "Epoch [1/2], Step [367/938], Loss: 0.2657\n",
      "Epoch [1/2], Step [368/938], Loss: 0.4136\n",
      "Epoch [1/2], Step [369/938], Loss: 0.1923\n",
      "Epoch [1/2], Step [370/938], Loss: 0.2522\n",
      "Epoch [1/2], Step [371/938], Loss: 0.1890\n",
      "Epoch [1/2], Step [372/938], Loss: 0.4654\n",
      "Epoch [1/2], Step [373/938], Loss: 0.2745\n",
      "Epoch [1/2], Step [374/938], Loss: 0.4393\n",
      "Epoch [1/2], Step [375/938], Loss: 0.3538\n",
      "Epoch [1/2], Step [376/938], Loss: 0.1774\n",
      "Epoch [1/2], Step [377/938], Loss: 0.2203\n",
      "Epoch [1/2], Step [378/938], Loss: 0.1697\n",
      "Epoch [1/2], Step [379/938], Loss: 0.4065\n",
      "Epoch [1/2], Step [380/938], Loss: 0.2929\n",
      "Epoch [1/2], Step [381/938], Loss: 0.2412\n",
      "Epoch [1/2], Step [382/938], Loss: 0.1023\n",
      "Epoch [1/2], Step [383/938], Loss: 0.3183\n",
      "Epoch [1/2], Step [384/938], Loss: 0.1343\n",
      "Epoch [1/2], Step [385/938], Loss: 0.2025\n",
      "Epoch [1/2], Step [386/938], Loss: 0.1555\n",
      "Epoch [1/2], Step [387/938], Loss: 0.2433\n",
      "Epoch [1/2], Step [388/938], Loss: 0.3786\n",
      "Epoch [1/2], Step [389/938], Loss: 0.3434\n",
      "Epoch [1/2], Step [390/938], Loss: 0.4036\n",
      "Epoch [1/2], Step [391/938], Loss: 0.3304\n",
      "Epoch [1/2], Step [392/938], Loss: 0.2063\n",
      "Epoch [1/2], Step [393/938], Loss: 0.2878\n",
      "Epoch [1/2], Step [394/938], Loss: 0.1508\n",
      "Epoch [1/2], Step [395/938], Loss: 0.2110\n",
      "Epoch [1/2], Step [396/938], Loss: 0.2925\n",
      "Epoch [1/2], Step [397/938], Loss: 0.2753\n",
      "Epoch [1/2], Step [398/938], Loss: 0.0532\n",
      "Epoch [1/2], Step [399/938], Loss: 0.2815\n",
      "Epoch [1/2], Step [400/938], Loss: 0.1658\n",
      "Epoch [1/2], Step [401/938], Loss: 0.2630\n",
      "Epoch [1/2], Step [402/938], Loss: 0.4131\n",
      "Epoch [1/2], Step [403/938], Loss: 0.2967\n",
      "Epoch [1/2], Step [404/938], Loss: 0.4690\n",
      "Epoch [1/2], Step [405/938], Loss: 0.2155\n",
      "Epoch [1/2], Step [406/938], Loss: 0.1979\n",
      "Epoch [1/2], Step [407/938], Loss: 0.1575\n",
      "Epoch [1/2], Step [408/938], Loss: 0.2020\n",
      "Epoch [1/2], Step [409/938], Loss: 0.5131\n",
      "Epoch [1/2], Step [410/938], Loss: 0.1578\n",
      "Epoch [1/2], Step [411/938], Loss: 0.4443\n",
      "Epoch [1/2], Step [412/938], Loss: 0.5166\n",
      "Epoch [1/2], Step [413/938], Loss: 0.2162\n",
      "Epoch [1/2], Step [414/938], Loss: 0.0579\n",
      "Epoch [1/2], Step [415/938], Loss: 0.4007\n",
      "Epoch [1/2], Step [416/938], Loss: 0.1487\n",
      "Epoch [1/2], Step [417/938], Loss: 0.2235\n",
      "Epoch [1/2], Step [418/938], Loss: 0.2776\n",
      "Epoch [1/2], Step [419/938], Loss: 0.1606\n",
      "Epoch [1/2], Step [420/938], Loss: 0.2330\n",
      "Epoch [1/2], Step [421/938], Loss: 0.2098\n",
      "Epoch [1/2], Step [422/938], Loss: 0.3546\n",
      "Epoch [1/2], Step [423/938], Loss: 0.1898\n",
      "Epoch [1/2], Step [424/938], Loss: 0.1597\n",
      "Epoch [1/2], Step [425/938], Loss: 0.0969\n",
      "Epoch [1/2], Step [426/938], Loss: 0.1373\n",
      "Epoch [1/2], Step [427/938], Loss: 0.1505\n",
      "Epoch [1/2], Step [428/938], Loss: 0.1804\n",
      "Epoch [1/2], Step [429/938], Loss: 0.1691\n",
      "Epoch [1/2], Step [430/938], Loss: 0.1688\n",
      "Epoch [1/2], Step [431/938], Loss: 0.2606\n",
      "Epoch [1/2], Step [432/938], Loss: 0.2158\n",
      "Epoch [1/2], Step [433/938], Loss: 0.1321\n",
      "Epoch [1/2], Step [434/938], Loss: 0.1486\n",
      "Epoch [1/2], Step [435/938], Loss: 0.2304\n",
      "Epoch [1/2], Step [436/938], Loss: 0.3796\n",
      "Epoch [1/2], Step [437/938], Loss: 0.1489\n",
      "Epoch [1/2], Step [438/938], Loss: 0.2283\n",
      "Epoch [1/2], Step [439/938], Loss: 0.1269\n",
      "Epoch [1/2], Step [440/938], Loss: 0.1384\n",
      "Epoch [1/2], Step [441/938], Loss: 0.1384\n",
      "Epoch [1/2], Step [442/938], Loss: 0.1814\n",
      "Epoch [1/2], Step [443/938], Loss: 0.4094\n",
      "Epoch [1/2], Step [444/938], Loss: 0.1895\n",
      "Epoch [1/2], Step [445/938], Loss: 0.4472\n",
      "Epoch [1/2], Step [446/938], Loss: 0.1760\n",
      "Epoch [1/2], Step [447/938], Loss: 0.2308\n",
      "Epoch [1/2], Step [448/938], Loss: 0.1988\n",
      "Epoch [1/2], Step [449/938], Loss: 0.2495\n",
      "Epoch [1/2], Step [450/938], Loss: 0.1818\n",
      "Epoch [1/2], Step [451/938], Loss: 0.1721\n",
      "Epoch [1/2], Step [452/938], Loss: 0.0975\n",
      "Epoch [1/2], Step [453/938], Loss: 0.5068\n",
      "Epoch [1/2], Step [454/938], Loss: 0.1834\n",
      "Epoch [1/2], Step [455/938], Loss: 0.2130\n",
      "Epoch [1/2], Step [456/938], Loss: 0.1505\n",
      "Epoch [1/2], Step [457/938], Loss: 0.3271\n",
      "Epoch [1/2], Step [458/938], Loss: 0.3595\n",
      "Epoch [1/2], Step [459/938], Loss: 0.2116\n",
      "Epoch [1/2], Step [460/938], Loss: 0.1419\n",
      "Epoch [1/2], Step [461/938], Loss: 0.3310\n",
      "Epoch [1/2], Step [462/938], Loss: 0.1716\n",
      "Epoch [1/2], Step [463/938], Loss: 0.2396\n",
      "Epoch [1/2], Step [464/938], Loss: 0.3007\n",
      "Epoch [1/2], Step [465/938], Loss: 0.2569\n",
      "Epoch [1/2], Step [466/938], Loss: 0.1062\n",
      "Epoch [1/2], Step [467/938], Loss: 0.2541\n",
      "Epoch [1/2], Step [468/938], Loss: 0.2486\n",
      "Epoch [1/2], Step [469/938], Loss: 0.1585\n",
      "Epoch [1/2], Step [470/938], Loss: 0.1302\n",
      "Epoch [1/2], Step [471/938], Loss: 0.2132\n",
      "Epoch [1/2], Step [472/938], Loss: 0.1313\n",
      "Epoch [1/2], Step [473/938], Loss: 0.1083\n",
      "Epoch [1/2], Step [474/938], Loss: 0.1798\n",
      "Epoch [1/2], Step [475/938], Loss: 0.0991\n",
      "Epoch [1/2], Step [476/938], Loss: 0.1320\n",
      "Epoch [1/2], Step [477/938], Loss: 0.2198\n",
      "Epoch [1/2], Step [478/938], Loss: 0.2022\n",
      "Epoch [1/2], Step [479/938], Loss: 0.1333\n",
      "Epoch [1/2], Step [480/938], Loss: 0.0831\n",
      "Epoch [1/2], Step [481/938], Loss: 0.1273\n",
      "Epoch [1/2], Step [482/938], Loss: 0.0840\n",
      "Epoch [1/2], Step [483/938], Loss: 0.3622\n",
      "Epoch [1/2], Step [484/938], Loss: 0.1673\n",
      "Epoch [1/2], Step [485/938], Loss: 0.1757\n",
      "Epoch [1/2], Step [486/938], Loss: 0.1220\n",
      "Epoch [1/2], Step [487/938], Loss: 0.1948\n",
      "Epoch [1/2], Step [488/938], Loss: 0.3745\n",
      "Epoch [1/2], Step [489/938], Loss: 0.3570\n",
      "Epoch [1/2], Step [490/938], Loss: 0.1454\n",
      "Epoch [1/2], Step [491/938], Loss: 0.3136\n",
      "Epoch [1/2], Step [492/938], Loss: 0.2114\n",
      "Epoch [1/2], Step [493/938], Loss: 0.2868\n",
      "Epoch [1/2], Step [494/938], Loss: 0.2659\n",
      "Epoch [1/2], Step [495/938], Loss: 0.5754\n",
      "Epoch [1/2], Step [496/938], Loss: 0.1620\n",
      "Epoch [1/2], Step [497/938], Loss: 0.4010\n",
      "Epoch [1/2], Step [498/938], Loss: 0.1772\n",
      "Epoch [1/2], Step [499/938], Loss: 0.2138\n",
      "Epoch [1/2], Step [500/938], Loss: 0.2220\n",
      "Epoch [1/2], Step [501/938], Loss: 0.4736\n",
      "Epoch [1/2], Step [502/938], Loss: 0.2831\n",
      "Epoch [1/2], Step [503/938], Loss: 0.0960\n",
      "Epoch [1/2], Step [504/938], Loss: 0.2632\n",
      "Epoch [1/2], Step [505/938], Loss: 0.2090\n",
      "Epoch [1/2], Step [506/938], Loss: 0.1376\n",
      "Epoch [1/2], Step [507/938], Loss: 0.2320\n",
      "Epoch [1/2], Step [508/938], Loss: 0.4406\n",
      "Epoch [1/2], Step [509/938], Loss: 0.1996\n",
      "Epoch [1/2], Step [510/938], Loss: 0.3380\n",
      "Epoch [1/2], Step [511/938], Loss: 0.1716\n",
      "Epoch [1/2], Step [512/938], Loss: 0.3313\n",
      "Epoch [1/2], Step [513/938], Loss: 0.3987\n",
      "Epoch [1/2], Step [514/938], Loss: 0.1876\n",
      "Epoch [1/2], Step [515/938], Loss: 0.1524\n",
      "Epoch [1/2], Step [516/938], Loss: 0.3614\n",
      "Epoch [1/2], Step [517/938], Loss: 0.1934\n",
      "Epoch [1/2], Step [518/938], Loss: 0.4314\n",
      "Epoch [1/2], Step [519/938], Loss: 0.2747\n",
      "Epoch [1/2], Step [520/938], Loss: 0.1269\n",
      "Epoch [1/2], Step [521/938], Loss: 0.3402\n",
      "Epoch [1/2], Step [522/938], Loss: 0.0829\n",
      "Epoch [1/2], Step [523/938], Loss: 0.1448\n",
      "Epoch [1/2], Step [524/938], Loss: 0.1601\n",
      "Epoch [1/2], Step [525/938], Loss: 0.1394\n",
      "Epoch [1/2], Step [526/938], Loss: 0.1629\n",
      "Epoch [1/2], Step [527/938], Loss: 0.1306\n",
      "Epoch [1/2], Step [528/938], Loss: 0.2583\n",
      "Epoch [1/2], Step [529/938], Loss: 0.0611\n",
      "Epoch [1/2], Step [530/938], Loss: 0.2604\n",
      "Epoch [1/2], Step [531/938], Loss: 0.0879\n",
      "Epoch [1/2], Step [532/938], Loss: 0.2473\n",
      "Epoch [1/2], Step [533/938], Loss: 0.1943\n",
      "Epoch [1/2], Step [534/938], Loss: 0.2003\n",
      "Epoch [1/2], Step [535/938], Loss: 0.0417\n",
      "Epoch [1/2], Step [536/938], Loss: 0.1421\n",
      "Epoch [1/2], Step [537/938], Loss: 0.1630\n",
      "Epoch [1/2], Step [538/938], Loss: 0.3176\n",
      "Epoch [1/2], Step [539/938], Loss: 0.1749\n",
      "Epoch [1/2], Step [540/938], Loss: 0.0845\n",
      "Epoch [1/2], Step [541/938], Loss: 0.3636\n",
      "Epoch [1/2], Step [542/938], Loss: 0.2319\n",
      "Epoch [1/2], Step [543/938], Loss: 0.3639\n",
      "Epoch [1/2], Step [544/938], Loss: 0.1867\n",
      "Epoch [1/2], Step [545/938], Loss: 0.1793\n",
      "Epoch [1/2], Step [546/938], Loss: 0.3195\n",
      "Epoch [1/2], Step [547/938], Loss: 0.2633\n",
      "Epoch [1/2], Step [548/938], Loss: 0.4149\n",
      "Epoch [1/2], Step [549/938], Loss: 0.1985\n",
      "Epoch [1/2], Step [550/938], Loss: 0.2879\n",
      "Epoch [1/2], Step [551/938], Loss: 0.2463\n",
      "Epoch [1/2], Step [552/938], Loss: 0.1539\n",
      "Epoch [1/2], Step [553/938], Loss: 0.2103\n",
      "Epoch [1/2], Step [554/938], Loss: 0.2743\n",
      "Epoch [1/2], Step [555/938], Loss: 0.1196\n",
      "Epoch [1/2], Step [556/938], Loss: 0.2842\n",
      "Epoch [1/2], Step [557/938], Loss: 0.1999\n",
      "Epoch [1/2], Step [558/938], Loss: 0.1123\n",
      "Epoch [1/2], Step [559/938], Loss: 0.1546\n",
      "Epoch [1/2], Step [560/938], Loss: 0.2196\n",
      "Epoch [1/2], Step [561/938], Loss: 0.2588\n",
      "Epoch [1/2], Step [562/938], Loss: 0.1866\n",
      "Epoch [1/2], Step [563/938], Loss: 0.1976\n",
      "Epoch [1/2], Step [564/938], Loss: 0.3040\n",
      "Epoch [1/2], Step [565/938], Loss: 0.1590\n",
      "Epoch [1/2], Step [566/938], Loss: 0.1202\n",
      "Epoch [1/2], Step [567/938], Loss: 0.2074\n",
      "Epoch [1/2], Step [568/938], Loss: 0.1355\n",
      "Epoch [1/2], Step [569/938], Loss: 0.2371\n",
      "Epoch [1/2], Step [570/938], Loss: 0.2420\n",
      "Epoch [1/2], Step [571/938], Loss: 0.1335\n",
      "Epoch [1/2], Step [572/938], Loss: 0.3021\n",
      "Epoch [1/2], Step [573/938], Loss: 0.0782\n",
      "Epoch [1/2], Step [574/938], Loss: 0.1109\n",
      "Epoch [1/2], Step [575/938], Loss: 0.1423\n",
      "Epoch [1/2], Step [576/938], Loss: 0.1743\n",
      "Epoch [1/2], Step [577/938], Loss: 0.1274\n",
      "Epoch [1/2], Step [578/938], Loss: 0.0488\n",
      "Epoch [1/2], Step [579/938], Loss: 0.1005\n",
      "Epoch [1/2], Step [580/938], Loss: 0.0592\n",
      "Epoch [1/2], Step [581/938], Loss: 0.1032\n",
      "Epoch [1/2], Step [582/938], Loss: 0.1938\n",
      "Epoch [1/2], Step [583/938], Loss: 0.0710\n",
      "Epoch [1/2], Step [584/938], Loss: 0.1224\n",
      "Epoch [1/2], Step [585/938], Loss: 0.1231\n",
      "Epoch [1/2], Step [586/938], Loss: 0.1765\n",
      "Epoch [1/2], Step [587/938], Loss: 0.2767\n",
      "Epoch [1/2], Step [588/938], Loss: 0.2591\n",
      "Epoch [1/2], Step [589/938], Loss: 0.0757\n",
      "Epoch [1/2], Step [590/938], Loss: 0.2275\n",
      "Epoch [1/2], Step [591/938], Loss: 0.3124\n",
      "Epoch [1/2], Step [592/938], Loss: 0.2069\n",
      "Epoch [1/2], Step [593/938], Loss: 0.1330\n",
      "Epoch [1/2], Step [594/938], Loss: 0.1690\n",
      "Epoch [1/2], Step [595/938], Loss: 0.0806\n",
      "Epoch [1/2], Step [596/938], Loss: 0.1958\n",
      "Epoch [1/2], Step [597/938], Loss: 0.0544\n",
      "Epoch [1/2], Step [598/938], Loss: 0.0967\n",
      "Epoch [1/2], Step [599/938], Loss: 0.2108\n",
      "Epoch [1/2], Step [600/938], Loss: 0.1526\n",
      "Epoch [1/2], Step [601/938], Loss: 0.3444\n",
      "Epoch [1/2], Step [602/938], Loss: 0.0575\n",
      "Epoch [1/2], Step [603/938], Loss: 0.0807\n",
      "Epoch [1/2], Step [604/938], Loss: 0.1177\n",
      "Epoch [1/2], Step [605/938], Loss: 0.3370\n",
      "Epoch [1/2], Step [606/938], Loss: 0.2909\n",
      "Epoch [1/2], Step [607/938], Loss: 0.1648\n",
      "Epoch [1/2], Step [608/938], Loss: 0.0889\n",
      "Epoch [1/2], Step [609/938], Loss: 0.1743\n",
      "Epoch [1/2], Step [610/938], Loss: 0.2095\n",
      "Epoch [1/2], Step [611/938], Loss: 0.2023\n",
      "Epoch [1/2], Step [612/938], Loss: 0.3714\n",
      "Epoch [1/2], Step [613/938], Loss: 0.3351\n",
      "Epoch [1/2], Step [614/938], Loss: 0.1502\n",
      "Epoch [1/2], Step [615/938], Loss: 0.2358\n",
      "Epoch [1/2], Step [616/938], Loss: 0.1441\n",
      "Epoch [1/2], Step [617/938], Loss: 0.1287\n",
      "Epoch [1/2], Step [618/938], Loss: 0.1164\n",
      "Epoch [1/2], Step [619/938], Loss: 0.1701\n",
      "Epoch [1/2], Step [620/938], Loss: 0.2214\n",
      "Epoch [1/2], Step [621/938], Loss: 0.1327\n",
      "Epoch [1/2], Step [622/938], Loss: 0.1128\n",
      "Epoch [1/2], Step [623/938], Loss: 0.3281\n",
      "Epoch [1/2], Step [624/938], Loss: 0.2868\n",
      "Epoch [1/2], Step [625/938], Loss: 0.1370\n",
      "Epoch [1/2], Step [626/938], Loss: 0.1990\n",
      "Epoch [1/2], Step [627/938], Loss: 0.1214\n",
      "Epoch [1/2], Step [628/938], Loss: 0.3231\n",
      "Epoch [1/2], Step [629/938], Loss: 0.0717\n",
      "Epoch [1/2], Step [630/938], Loss: 0.1599\n",
      "Epoch [1/2], Step [631/938], Loss: 0.1368\n",
      "Epoch [1/2], Step [632/938], Loss: 0.1612\n",
      "Epoch [1/2], Step [633/938], Loss: 0.2579\n",
      "Epoch [1/2], Step [634/938], Loss: 0.1635\n",
      "Epoch [1/2], Step [635/938], Loss: 0.1592\n",
      "Epoch [1/2], Step [636/938], Loss: 0.0622\n",
      "Epoch [1/2], Step [637/938], Loss: 0.1113\n",
      "Epoch [1/2], Step [638/938], Loss: 0.2094\n",
      "Epoch [1/2], Step [639/938], Loss: 0.1303\n",
      "Epoch [1/2], Step [640/938], Loss: 0.0654\n",
      "Epoch [1/2], Step [641/938], Loss: 0.0420\n",
      "Epoch [1/2], Step [642/938], Loss: 0.0430\n",
      "Epoch [1/2], Step [643/938], Loss: 0.0932\n",
      "Epoch [1/2], Step [644/938], Loss: 0.0888\n",
      "Epoch [1/2], Step [645/938], Loss: 0.1443\n",
      "Epoch [1/2], Step [646/938], Loss: 0.1646\n",
      "Epoch [1/2], Step [647/938], Loss: 0.2171\n",
      "Epoch [1/2], Step [648/938], Loss: 0.0753\n",
      "Epoch [1/2], Step [649/938], Loss: 0.1208\n",
      "Epoch [1/2], Step [650/938], Loss: 0.1946\n",
      "Epoch [1/2], Step [651/938], Loss: 0.2681\n",
      "Epoch [1/2], Step [652/938], Loss: 0.3991\n",
      "Epoch [1/2], Step [653/938], Loss: 0.2230\n",
      "Epoch [1/2], Step [654/938], Loss: 0.0725\n",
      "Epoch [1/2], Step [655/938], Loss: 0.0746\n",
      "Epoch [1/2], Step [656/938], Loss: 0.2171\n",
      "Epoch [1/2], Step [657/938], Loss: 0.0976\n",
      "Epoch [1/2], Step [658/938], Loss: 0.2056\n",
      "Epoch [1/2], Step [659/938], Loss: 0.1767\n",
      "Epoch [1/2], Step [660/938], Loss: 0.2273\n",
      "Epoch [1/2], Step [661/938], Loss: 0.2448\n",
      "Epoch [1/2], Step [662/938], Loss: 0.1437\n",
      "Epoch [1/2], Step [663/938], Loss: 0.0963\n",
      "Epoch [1/2], Step [664/938], Loss: 0.0890\n",
      "Epoch [1/2], Step [665/938], Loss: 0.1130\n",
      "Epoch [1/2], Step [666/938], Loss: 0.2044\n",
      "Epoch [1/2], Step [667/938], Loss: 0.2858\n",
      "Epoch [1/2], Step [668/938], Loss: 0.1555\n",
      "Epoch [1/2], Step [669/938], Loss: 0.0799\n",
      "Epoch [1/2], Step [670/938], Loss: 0.1101\n",
      "Epoch [1/2], Step [671/938], Loss: 0.1786\n",
      "Epoch [1/2], Step [672/938], Loss: 0.2754\n",
      "Epoch [1/2], Step [673/938], Loss: 0.3201\n",
      "Epoch [1/2], Step [674/938], Loss: 0.1337\n",
      "Epoch [1/2], Step [675/938], Loss: 0.0960\n",
      "Epoch [1/2], Step [676/938], Loss: 0.4392\n",
      "Epoch [1/2], Step [677/938], Loss: 0.1377\n",
      "Epoch [1/2], Step [678/938], Loss: 0.2529\n",
      "Epoch [1/2], Step [679/938], Loss: 0.1632\n",
      "Epoch [1/2], Step [680/938], Loss: 0.1478\n",
      "Epoch [1/2], Step [681/938], Loss: 0.1331\n",
      "Epoch [1/2], Step [682/938], Loss: 0.1478\n",
      "Epoch [1/2], Step [683/938], Loss: 0.1230\n",
      "Epoch [1/2], Step [684/938], Loss: 0.1724\n",
      "Epoch [1/2], Step [685/938], Loss: 0.2138\n",
      "Epoch [1/2], Step [686/938], Loss: 0.1843\n",
      "Epoch [1/2], Step [687/938], Loss: 0.0951\n",
      "Epoch [1/2], Step [688/938], Loss: 0.1347\n",
      "Epoch [1/2], Step [689/938], Loss: 0.2858\n",
      "Epoch [1/2], Step [690/938], Loss: 0.4637\n",
      "Epoch [1/2], Step [691/938], Loss: 0.0399\n",
      "Epoch [1/2], Step [692/938], Loss: 0.3552\n",
      "Epoch [1/2], Step [693/938], Loss: 0.3002\n",
      "Epoch [1/2], Step [694/938], Loss: 0.0784\n",
      "Epoch [1/2], Step [695/938], Loss: 0.0861\n",
      "Epoch [1/2], Step [696/938], Loss: 0.1484\n",
      "Epoch [1/2], Step [697/938], Loss: 0.2619\n",
      "Epoch [1/2], Step [698/938], Loss: 0.1713\n",
      "Epoch [1/2], Step [699/938], Loss: 0.2043\n",
      "Epoch [1/2], Step [700/938], Loss: 0.1569\n",
      "Epoch [1/2], Step [701/938], Loss: 0.2120\n",
      "Epoch [1/2], Step [702/938], Loss: 0.1043\n",
      "Epoch [1/2], Step [703/938], Loss: 0.0905\n",
      "Epoch [1/2], Step [704/938], Loss: 0.1369\n",
      "Epoch [1/2], Step [705/938], Loss: 0.3459\n",
      "Epoch [1/2], Step [706/938], Loss: 0.2816\n",
      "Epoch [1/2], Step [707/938], Loss: 0.2453\n",
      "Epoch [1/2], Step [708/938], Loss: 0.1988\n",
      "Epoch [1/2], Step [709/938], Loss: 0.1156\n",
      "Epoch [1/2], Step [710/938], Loss: 0.1377\n",
      "Epoch [1/2], Step [711/938], Loss: 0.1118\n",
      "Epoch [1/2], Step [712/938], Loss: 0.1394\n",
      "Epoch [1/2], Step [713/938], Loss: 0.3680\n",
      "Epoch [1/2], Step [714/938], Loss: 0.2010\n",
      "Epoch [1/2], Step [715/938], Loss: 0.1779\n",
      "Epoch [1/2], Step [716/938], Loss: 0.2075\n",
      "Epoch [1/2], Step [717/938], Loss: 0.1939\n",
      "Epoch [1/2], Step [718/938], Loss: 0.0775\n",
      "Epoch [1/2], Step [719/938], Loss: 0.0800\n",
      "Epoch [1/2], Step [720/938], Loss: 0.1065\n",
      "Epoch [1/2], Step [721/938], Loss: 0.0988\n",
      "Epoch [1/2], Step [722/938], Loss: 0.1589\n",
      "Epoch [1/2], Step [723/938], Loss: 0.2756\n",
      "Epoch [1/2], Step [724/938], Loss: 0.3278\n",
      "Epoch [1/2], Step [725/938], Loss: 0.1366\n",
      "Epoch [1/2], Step [726/938], Loss: 0.1561\n",
      "Epoch [1/2], Step [727/938], Loss: 0.1077\n",
      "Epoch [1/2], Step [728/938], Loss: 0.1771\n",
      "Epoch [1/2], Step [729/938], Loss: 0.2094\n",
      "Epoch [1/2], Step [730/938], Loss: 0.3209\n",
      "Epoch [1/2], Step [731/938], Loss: 0.1036\n",
      "Epoch [1/2], Step [732/938], Loss: 0.1645\n",
      "Epoch [1/2], Step [733/938], Loss: 0.1734\n",
      "Epoch [1/2], Step [734/938], Loss: 0.2188\n",
      "Epoch [1/2], Step [735/938], Loss: 0.3416\n",
      "Epoch [1/2], Step [736/938], Loss: 0.1390\n",
      "Epoch [1/2], Step [737/938], Loss: 0.1026\n",
      "Epoch [1/2], Step [738/938], Loss: 0.2199\n",
      "Epoch [1/2], Step [739/938], Loss: 0.2438\n",
      "Epoch [1/2], Step [740/938], Loss: 0.1266\n",
      "Epoch [1/2], Step [741/938], Loss: 0.1604\n",
      "Epoch [1/2], Step [742/938], Loss: 0.2573\n",
      "Epoch [1/2], Step [743/938], Loss: 0.1754\n",
      "Epoch [1/2], Step [744/938], Loss: 0.1589\n",
      "Epoch [1/2], Step [745/938], Loss: 0.2106\n",
      "Epoch [1/2], Step [746/938], Loss: 0.1752\n",
      "Epoch [1/2], Step [747/938], Loss: 0.1014\n",
      "Epoch [1/2], Step [748/938], Loss: 0.1062\n",
      "Epoch [1/2], Step [749/938], Loss: 0.1538\n",
      "Epoch [1/2], Step [750/938], Loss: 0.1015\n",
      "Epoch [1/2], Step [751/938], Loss: 0.1121\n",
      "Epoch [1/2], Step [752/938], Loss: 0.1962\n",
      "Epoch [1/2], Step [753/938], Loss: 0.2725\n",
      "Epoch [1/2], Step [754/938], Loss: 0.4189\n",
      "Epoch [1/2], Step [755/938], Loss: 0.0676\n",
      "Epoch [1/2], Step [756/938], Loss: 0.0882\n",
      "Epoch [1/2], Step [757/938], Loss: 0.1603\n",
      "Epoch [1/2], Step [758/938], Loss: 0.0625\n",
      "Epoch [1/2], Step [759/938], Loss: 0.0271\n",
      "Epoch [1/2], Step [760/938], Loss: 0.1186\n",
      "Epoch [1/2], Step [761/938], Loss: 0.1423\n",
      "Epoch [1/2], Step [762/938], Loss: 0.1731\n",
      "Epoch [1/2], Step [763/938], Loss: 0.3019\n",
      "Epoch [1/2], Step [764/938], Loss: 0.0939\n",
      "Epoch [1/2], Step [765/938], Loss: 0.1748\n",
      "Epoch [1/2], Step [766/938], Loss: 0.1301\n",
      "Epoch [1/2], Step [767/938], Loss: 0.0970\n",
      "Epoch [1/2], Step [768/938], Loss: 0.2877\n",
      "Epoch [1/2], Step [769/938], Loss: 0.1900\n",
      "Epoch [1/2], Step [770/938], Loss: 0.1806\n",
      "Epoch [1/2], Step [771/938], Loss: 0.0535\n",
      "Epoch [1/2], Step [772/938], Loss: 0.1366\n",
      "Epoch [1/2], Step [773/938], Loss: 0.0794\n",
      "Epoch [1/2], Step [774/938], Loss: 0.1552\n",
      "Epoch [1/2], Step [775/938], Loss: 0.1883\n",
      "Epoch [1/2], Step [776/938], Loss: 0.0790\n",
      "Epoch [1/2], Step [777/938], Loss: 0.1880\n",
      "Epoch [1/2], Step [778/938], Loss: 0.0276\n",
      "Epoch [1/2], Step [779/938], Loss: 0.0197\n",
      "Epoch [1/2], Step [780/938], Loss: 0.1709\n",
      "Epoch [1/2], Step [781/938], Loss: 0.1138\n",
      "Epoch [1/2], Step [782/938], Loss: 0.1525\n",
      "Epoch [1/2], Step [783/938], Loss: 0.1015\n",
      "Epoch [1/2], Step [784/938], Loss: 0.0747\n",
      "Epoch [1/2], Step [785/938], Loss: 0.1164\n",
      "Epoch [1/2], Step [786/938], Loss: 0.1190\n",
      "Epoch [1/2], Step [787/938], Loss: 0.1169\n",
      "Epoch [1/2], Step [788/938], Loss: 0.1369\n",
      "Epoch [1/2], Step [789/938], Loss: 0.1301\n",
      "Epoch [1/2], Step [790/938], Loss: 0.1111\n",
      "Epoch [1/2], Step [791/938], Loss: 0.0871\n",
      "Epoch [1/2], Step [792/938], Loss: 0.0668\n",
      "Epoch [1/2], Step [793/938], Loss: 0.1631\n",
      "Epoch [1/2], Step [794/938], Loss: 0.1459\n",
      "Epoch [1/2], Step [795/938], Loss: 0.3792\n",
      "Epoch [1/2], Step [796/938], Loss: 0.0385\n",
      "Epoch [1/2], Step [797/938], Loss: 0.2442\n",
      "Epoch [1/2], Step [798/938], Loss: 0.1965\n",
      "Epoch [1/2], Step [799/938], Loss: 0.1847\n",
      "Epoch [1/2], Step [800/938], Loss: 0.0871\n",
      "Epoch [1/2], Step [801/938], Loss: 0.1098\n",
      "Epoch [1/2], Step [802/938], Loss: 0.1578\n",
      "Epoch [1/2], Step [803/938], Loss: 0.0738\n",
      "Epoch [1/2], Step [804/938], Loss: 0.1650\n",
      "Epoch [1/2], Step [805/938], Loss: 0.2210\n",
      "Epoch [1/2], Step [806/938], Loss: 0.1802\n",
      "Epoch [1/2], Step [807/938], Loss: 0.1418\n",
      "Epoch [1/2], Step [808/938], Loss: 0.0512\n",
      "Epoch [1/2], Step [809/938], Loss: 0.0549\n",
      "Epoch [1/2], Step [810/938], Loss: 0.1731\n",
      "Epoch [1/2], Step [811/938], Loss: 0.0919\n",
      "Epoch [1/2], Step [812/938], Loss: 0.1307\n",
      "Epoch [1/2], Step [813/938], Loss: 0.0638\n",
      "Epoch [1/2], Step [814/938], Loss: 0.1327\n",
      "Epoch [1/2], Step [815/938], Loss: 0.1109\n",
      "Epoch [1/2], Step [816/938], Loss: 0.0853\n",
      "Epoch [1/2], Step [817/938], Loss: 0.1061\n",
      "Epoch [1/2], Step [818/938], Loss: 0.3109\n",
      "Epoch [1/2], Step [819/938], Loss: 0.1083\n",
      "Epoch [1/2], Step [820/938], Loss: 0.3730\n",
      "Epoch [1/2], Step [821/938], Loss: 0.1834\n",
      "Epoch [1/2], Step [822/938], Loss: 0.1602\n",
      "Epoch [1/2], Step [823/938], Loss: 0.0892\n",
      "Epoch [1/2], Step [824/938], Loss: 0.1881\n",
      "Epoch [1/2], Step [825/938], Loss: 0.3068\n",
      "Epoch [1/2], Step [826/938], Loss: 0.2542\n",
      "Epoch [1/2], Step [827/938], Loss: 0.4138\n",
      "Epoch [1/2], Step [828/938], Loss: 0.1723\n",
      "Epoch [1/2], Step [829/938], Loss: 0.2106\n",
      "Epoch [1/2], Step [830/938], Loss: 0.3673\n",
      "Epoch [1/2], Step [831/938], Loss: 0.2154\n",
      "Epoch [1/2], Step [832/938], Loss: 0.1097\n",
      "Epoch [1/2], Step [833/938], Loss: 0.0571\n",
      "Epoch [1/2], Step [834/938], Loss: 0.1204\n",
      "Epoch [1/2], Step [835/938], Loss: 0.2065\n",
      "Epoch [1/2], Step [836/938], Loss: 0.1112\n",
      "Epoch [1/2], Step [837/938], Loss: 0.5709\n",
      "Epoch [1/2], Step [838/938], Loss: 0.1400\n",
      "Epoch [1/2], Step [839/938], Loss: 0.1136\n",
      "Epoch [1/2], Step [840/938], Loss: 0.1310\n",
      "Epoch [1/2], Step [841/938], Loss: 0.2896\n",
      "Epoch [1/2], Step [842/938], Loss: 0.2638\n",
      "Epoch [1/2], Step [843/938], Loss: 0.1242\n",
      "Epoch [1/2], Step [844/938], Loss: 0.1888\n",
      "Epoch [1/2], Step [845/938], Loss: 0.1944\n",
      "Epoch [1/2], Step [846/938], Loss: 0.1266\n",
      "Epoch [1/2], Step [847/938], Loss: 0.2798\n",
      "Epoch [1/2], Step [848/938], Loss: 0.0604\n",
      "Epoch [1/2], Step [849/938], Loss: 0.2156\n",
      "Epoch [1/2], Step [850/938], Loss: 0.2380\n",
      "Epoch [1/2], Step [851/938], Loss: 0.1005\n",
      "Epoch [1/2], Step [852/938], Loss: 0.1446\n",
      "Epoch [1/2], Step [853/938], Loss: 0.1136\n",
      "Epoch [1/2], Step [854/938], Loss: 0.0654\n",
      "Epoch [1/2], Step [855/938], Loss: 0.0777\n",
      "Epoch [1/2], Step [856/938], Loss: 0.0156\n",
      "Epoch [1/2], Step [857/938], Loss: 0.2235\n",
      "Epoch [1/2], Step [858/938], Loss: 0.1607\n",
      "Epoch [1/2], Step [859/938], Loss: 0.1849\n",
      "Epoch [1/2], Step [860/938], Loss: 0.0381\n",
      "Epoch [1/2], Step [861/938], Loss: 0.0624\n",
      "Epoch [1/2], Step [862/938], Loss: 0.2320\n",
      "Epoch [1/2], Step [863/938], Loss: 0.2011\n",
      "Epoch [1/2], Step [864/938], Loss: 0.0290\n",
      "Epoch [1/2], Step [865/938], Loss: 0.5089\n",
      "Epoch [1/2], Step [866/938], Loss: 0.1524\n",
      "Epoch [1/2], Step [867/938], Loss: 0.0498\n",
      "Epoch [1/2], Step [868/938], Loss: 0.1551\n",
      "Epoch [1/2], Step [869/938], Loss: 0.0799\n",
      "Epoch [1/2], Step [870/938], Loss: 0.1573\n",
      "Epoch [1/2], Step [871/938], Loss: 0.0808\n",
      "Epoch [1/2], Step [872/938], Loss: 0.1035\n",
      "Epoch [1/2], Step [873/938], Loss: 0.1585\n",
      "Epoch [1/2], Step [874/938], Loss: 0.3530\n",
      "Epoch [1/2], Step [875/938], Loss: 0.1351\n",
      "Epoch [1/2], Step [876/938], Loss: 0.1073\n",
      "Epoch [1/2], Step [877/938], Loss: 0.0824\n",
      "Epoch [1/2], Step [878/938], Loss: 0.0837\n",
      "Epoch [1/2], Step [879/938], Loss: 0.1331\n",
      "Epoch [1/2], Step [880/938], Loss: 0.2105\n",
      "Epoch [1/2], Step [881/938], Loss: 0.3986\n",
      "Epoch [1/2], Step [882/938], Loss: 0.1503\n",
      "Epoch [1/2], Step [883/938], Loss: 0.1859\n",
      "Epoch [1/2], Step [884/938], Loss: 0.4145\n",
      "Epoch [1/2], Step [885/938], Loss: 0.1739\n",
      "Epoch [1/2], Step [886/938], Loss: 0.1847\n",
      "Epoch [1/2], Step [887/938], Loss: 0.0645\n",
      "Epoch [1/2], Step [888/938], Loss: 0.1673\n",
      "Epoch [1/2], Step [889/938], Loss: 0.0939\n",
      "Epoch [1/2], Step [890/938], Loss: 0.1475\n",
      "Epoch [1/2], Step [891/938], Loss: 0.2283\n",
      "Epoch [1/2], Step [892/938], Loss: 0.2834\n",
      "Epoch [1/2], Step [893/938], Loss: 0.1488\n",
      "Epoch [1/2], Step [894/938], Loss: 0.1057\n",
      "Epoch [1/2], Step [895/938], Loss: 0.1982\n",
      "Epoch [1/2], Step [896/938], Loss: 0.0337\n",
      "Epoch [1/2], Step [897/938], Loss: 0.1393\n",
      "Epoch [1/2], Step [898/938], Loss: 0.1179\n",
      "Epoch [1/2], Step [899/938], Loss: 0.1280\n",
      "Epoch [1/2], Step [900/938], Loss: 0.2534\n",
      "Epoch [1/2], Step [901/938], Loss: 0.0992\n",
      "Epoch [1/2], Step [902/938], Loss: 0.1553\n",
      "Epoch [1/2], Step [903/938], Loss: 0.0763\n",
      "Epoch [1/2], Step [904/938], Loss: 0.0438\n",
      "Epoch [1/2], Step [905/938], Loss: 0.1717\n",
      "Epoch [1/2], Step [906/938], Loss: 0.4375\n",
      "Epoch [1/2], Step [907/938], Loss: 0.0492\n",
      "Epoch [1/2], Step [908/938], Loss: 0.1226\n",
      "Epoch [1/2], Step [909/938], Loss: 0.0447\n",
      "Epoch [1/2], Step [910/938], Loss: 0.1969\n",
      "Epoch [1/2], Step [911/938], Loss: 0.1400\n",
      "Epoch [1/2], Step [912/938], Loss: 0.2241\n",
      "Epoch [1/2], Step [913/938], Loss: 0.2521\n",
      "Epoch [1/2], Step [914/938], Loss: 0.0904\n",
      "Epoch [1/2], Step [915/938], Loss: 0.1911\n",
      "Epoch [1/2], Step [916/938], Loss: 0.1112\n",
      "Epoch [1/2], Step [917/938], Loss: 0.0809\n",
      "Epoch [1/2], Step [918/938], Loss: 0.1483\n",
      "Epoch [1/2], Step [919/938], Loss: 0.1364\n",
      "Epoch [1/2], Step [920/938], Loss: 0.2971\n",
      "Epoch [1/2], Step [921/938], Loss: 0.2139\n",
      "Epoch [1/2], Step [922/938], Loss: 0.2703\n",
      "Epoch [1/2], Step [923/938], Loss: 0.2471\n",
      "Epoch [1/2], Step [924/938], Loss: 0.0862\n",
      "Epoch [1/2], Step [925/938], Loss: 0.1659\n",
      "Epoch [1/2], Step [926/938], Loss: 0.1947\n",
      "Epoch [1/2], Step [927/938], Loss: 0.2923\n",
      "Epoch [1/2], Step [928/938], Loss: 0.1370\n",
      "Epoch [1/2], Step [929/938], Loss: 0.3762\n",
      "Epoch [1/2], Step [930/938], Loss: 0.2285\n",
      "Epoch [1/2], Step [931/938], Loss: 0.1501\n",
      "Epoch [1/2], Step [932/938], Loss: 0.1250\n",
      "Epoch [1/2], Step [933/938], Loss: 0.1059\n",
      "Epoch [1/2], Step [934/938], Loss: 0.0545\n",
      "Epoch [1/2], Step [935/938], Loss: 0.0509\n",
      "Epoch [1/2], Step [936/938], Loss: 0.0833\n",
      "Epoch [1/2], Step [937/938], Loss: 0.2325\n",
      "Epoch [1/2], Step [938/938], Loss: 0.0847\n",
      "Epoch [2/2], Step [1/938], Loss: 0.2635\n",
      "Epoch [2/2], Step [2/938], Loss: 0.1105\n",
      "Epoch [2/2], Step [3/938], Loss: 0.0862\n",
      "Epoch [2/2], Step [4/938], Loss: 0.0801\n",
      "Epoch [2/2], Step [5/938], Loss: 0.1915\n",
      "Epoch [2/2], Step [6/938], Loss: 0.3584\n",
      "Epoch [2/2], Step [7/938], Loss: 0.0333\n",
      "Epoch [2/2], Step [8/938], Loss: 0.1819\n",
      "Epoch [2/2], Step [9/938], Loss: 0.2324\n",
      "Epoch [2/2], Step [10/938], Loss: 0.2214\n",
      "Epoch [2/2], Step [11/938], Loss: 0.1313\n",
      "Epoch [2/2], Step [12/938], Loss: 0.1730\n",
      "Epoch [2/2], Step [13/938], Loss: 0.0406\n",
      "Epoch [2/2], Step [14/938], Loss: 0.0651\n",
      "Epoch [2/2], Step [15/938], Loss: 0.2600\n",
      "Epoch [2/2], Step [16/938], Loss: 0.2326\n",
      "Epoch [2/2], Step [17/938], Loss: 0.3135\n",
      "Epoch [2/2], Step [18/938], Loss: 0.1945\n",
      "Epoch [2/2], Step [19/938], Loss: 0.1330\n",
      "Epoch [2/2], Step [20/938], Loss: 0.1425\n",
      "Epoch [2/2], Step [21/938], Loss: 0.1436\n",
      "Epoch [2/2], Step [22/938], Loss: 0.2750\n",
      "Epoch [2/2], Step [23/938], Loss: 0.3549\n",
      "Epoch [2/2], Step [24/938], Loss: 0.1837\n",
      "Epoch [2/2], Step [25/938], Loss: 0.0348\n",
      "Epoch [2/2], Step [26/938], Loss: 0.0771\n",
      "Epoch [2/2], Step [27/938], Loss: 0.0713\n",
      "Epoch [2/2], Step [28/938], Loss: 0.0634\n",
      "Epoch [2/2], Step [29/938], Loss: 0.1830\n",
      "Epoch [2/2], Step [30/938], Loss: 0.2210\n",
      "Epoch [2/2], Step [31/938], Loss: 0.1709\n",
      "Epoch [2/2], Step [32/938], Loss: 0.1708\n",
      "Epoch [2/2], Step [33/938], Loss: 0.1257\n",
      "Epoch [2/2], Step [34/938], Loss: 0.2332\n",
      "Epoch [2/2], Step [35/938], Loss: 0.1657\n",
      "Epoch [2/2], Step [36/938], Loss: 0.1444\n",
      "Epoch [2/2], Step [37/938], Loss: 0.1137\n",
      "Epoch [2/2], Step [38/938], Loss: 0.0545\n",
      "Epoch [2/2], Step [39/938], Loss: 0.0316\n",
      "Epoch [2/2], Step [40/938], Loss: 0.1256\n",
      "Epoch [2/2], Step [41/938], Loss: 0.2592\n",
      "Epoch [2/2], Step [42/938], Loss: 0.1437\n",
      "Epoch [2/2], Step [43/938], Loss: 0.0830\n",
      "Epoch [2/2], Step [44/938], Loss: 0.1097\n",
      "Epoch [2/2], Step [45/938], Loss: 0.1131\n",
      "Epoch [2/2], Step [46/938], Loss: 0.2885\n",
      "Epoch [2/2], Step [47/938], Loss: 0.1610\n",
      "Epoch [2/2], Step [48/938], Loss: 0.0852\n",
      "Epoch [2/2], Step [49/938], Loss: 0.2175\n",
      "Epoch [2/2], Step [50/938], Loss: 0.0531\n",
      "Epoch [2/2], Step [51/938], Loss: 0.1077\n",
      "Epoch [2/2], Step [52/938], Loss: 0.1430\n",
      "Epoch [2/2], Step [53/938], Loss: 0.1148\n",
      "Epoch [2/2], Step [54/938], Loss: 0.0515\n",
      "Epoch [2/2], Step [55/938], Loss: 0.1572\n",
      "Epoch [2/2], Step [56/938], Loss: 0.1862\n",
      "Epoch [2/2], Step [57/938], Loss: 0.0511\n",
      "Epoch [2/2], Step [58/938], Loss: 0.1114\n",
      "Epoch [2/2], Step [59/938], Loss: 0.0529\n",
      "Epoch [2/2], Step [60/938], Loss: 0.2447\n",
      "Epoch [2/2], Step [61/938], Loss: 0.0567\n",
      "Epoch [2/2], Step [62/938], Loss: 0.0641\n",
      "Epoch [2/2], Step [63/938], Loss: 0.0185\n",
      "Epoch [2/2], Step [64/938], Loss: 0.1498\n",
      "Epoch [2/2], Step [65/938], Loss: 0.0390\n",
      "Epoch [2/2], Step [66/938], Loss: 0.1400\n",
      "Epoch [2/2], Step [67/938], Loss: 0.1685\n",
      "Epoch [2/2], Step [68/938], Loss: 0.2191\n",
      "Epoch [2/2], Step [69/938], Loss: 0.0570\n",
      "Epoch [2/2], Step [70/938], Loss: 0.0967\n",
      "Epoch [2/2], Step [71/938], Loss: 0.2883\n",
      "Epoch [2/2], Step [72/938], Loss: 0.1943\n",
      "Epoch [2/2], Step [73/938], Loss: 0.0702\n",
      "Epoch [2/2], Step [74/938], Loss: 0.1157\n",
      "Epoch [2/2], Step [75/938], Loss: 0.2816\n",
      "Epoch [2/2], Step [76/938], Loss: 0.3714\n",
      "Epoch [2/2], Step [77/938], Loss: 0.1347\n",
      "Epoch [2/2], Step [78/938], Loss: 0.0960\n",
      "Epoch [2/2], Step [79/938], Loss: 0.0914\n",
      "Epoch [2/2], Step [80/938], Loss: 0.1881\n",
      "Epoch [2/2], Step [81/938], Loss: 0.2011\n",
      "Epoch [2/2], Step [82/938], Loss: 0.0682\n",
      "Epoch [2/2], Step [83/938], Loss: 0.0421\n",
      "Epoch [2/2], Step [84/938], Loss: 0.0891\n",
      "Epoch [2/2], Step [85/938], Loss: 0.1108\n",
      "Epoch [2/2], Step [86/938], Loss: 0.0489\n",
      "Epoch [2/2], Step [87/938], Loss: 0.0929\n",
      "Epoch [2/2], Step [88/938], Loss: 0.2888\n",
      "Epoch [2/2], Step [89/938], Loss: 0.0831\n",
      "Epoch [2/2], Step [90/938], Loss: 0.0978\n",
      "Epoch [2/2], Step [91/938], Loss: 0.3425\n",
      "Epoch [2/2], Step [92/938], Loss: 0.2355\n",
      "Epoch [2/2], Step [93/938], Loss: 0.1248\n",
      "Epoch [2/2], Step [94/938], Loss: 0.2326\n",
      "Epoch [2/2], Step [95/938], Loss: 0.1711\n",
      "Epoch [2/2], Step [96/938], Loss: 0.2444\n",
      "Epoch [2/2], Step [97/938], Loss: 0.2195\n",
      "Epoch [2/2], Step [98/938], Loss: 0.0634\n",
      "Epoch [2/2], Step [99/938], Loss: 0.1161\n",
      "Epoch [2/2], Step [100/938], Loss: 0.2012\n",
      "Epoch [2/2], Step [101/938], Loss: 0.1541\n",
      "Epoch [2/2], Step [102/938], Loss: 0.0942\n",
      "Epoch [2/2], Step [103/938], Loss: 0.1186\n",
      "Epoch [2/2], Step [104/938], Loss: 0.1586\n",
      "Epoch [2/2], Step [105/938], Loss: 0.0837\n",
      "Epoch [2/2], Step [106/938], Loss: 0.2555\n",
      "Epoch [2/2], Step [107/938], Loss: 0.0816\n",
      "Epoch [2/2], Step [108/938], Loss: 0.0515\n",
      "Epoch [2/2], Step [109/938], Loss: 0.3957\n",
      "Epoch [2/2], Step [110/938], Loss: 0.0584\n",
      "Epoch [2/2], Step [111/938], Loss: 0.0596\n",
      "Epoch [2/2], Step [112/938], Loss: 0.1552\n",
      "Epoch [2/2], Step [113/938], Loss: 0.0694\n",
      "Epoch [2/2], Step [114/938], Loss: 0.0628\n",
      "Epoch [2/2], Step [115/938], Loss: 0.1704\n",
      "Epoch [2/2], Step [116/938], Loss: 0.1324\n",
      "Epoch [2/2], Step [117/938], Loss: 0.1342\n",
      "Epoch [2/2], Step [118/938], Loss: 0.1005\n",
      "Epoch [2/2], Step [119/938], Loss: 0.0800\n",
      "Epoch [2/2], Step [120/938], Loss: 0.0976\n",
      "Epoch [2/2], Step [121/938], Loss: 0.0522\n",
      "Epoch [2/2], Step [122/938], Loss: 0.2414\n",
      "Epoch [2/2], Step [123/938], Loss: 0.1872\n",
      "Epoch [2/2], Step [124/938], Loss: 0.1683\n",
      "Epoch [2/2], Step [125/938], Loss: 0.0495\n",
      "Epoch [2/2], Step [126/938], Loss: 0.1290\n",
      "Epoch [2/2], Step [127/938], Loss: 0.1158\n",
      "Epoch [2/2], Step [128/938], Loss: 0.0951\n",
      "Epoch [2/2], Step [129/938], Loss: 0.0385\n",
      "Epoch [2/2], Step [130/938], Loss: 0.2266\n",
      "Epoch [2/2], Step [131/938], Loss: 0.1996\n",
      "Epoch [2/2], Step [132/938], Loss: 0.2694\n",
      "Epoch [2/2], Step [133/938], Loss: 0.0459\n",
      "Epoch [2/2], Step [134/938], Loss: 0.2260\n",
      "Epoch [2/2], Step [135/938], Loss: 0.1227\n",
      "Epoch [2/2], Step [136/938], Loss: 0.0704\n",
      "Epoch [2/2], Step [137/938], Loss: 0.1076\n",
      "Epoch [2/2], Step [138/938], Loss: 0.0073\n",
      "Epoch [2/2], Step [139/938], Loss: 0.0696\n",
      "Epoch [2/2], Step [140/938], Loss: 0.0499\n",
      "Epoch [2/2], Step [141/938], Loss: 0.0365\n",
      "Epoch [2/2], Step [142/938], Loss: 0.0566\n",
      "Epoch [2/2], Step [143/938], Loss: 0.1048\n",
      "Epoch [2/2], Step [144/938], Loss: 0.0423\n",
      "Epoch [2/2], Step [145/938], Loss: 0.1606\n",
      "Epoch [2/2], Step [146/938], Loss: 0.1076\n",
      "Epoch [2/2], Step [147/938], Loss: 0.1209\n",
      "Epoch [2/2], Step [148/938], Loss: 0.1673\n",
      "Epoch [2/2], Step [149/938], Loss: 0.0093\n",
      "Epoch [2/2], Step [150/938], Loss: 0.1091\n",
      "Epoch [2/2], Step [151/938], Loss: 0.0257\n",
      "Epoch [2/2], Step [152/938], Loss: 0.1634\n",
      "Epoch [2/2], Step [153/938], Loss: 0.0862\n",
      "Epoch [2/2], Step [154/938], Loss: 0.1800\n",
      "Epoch [2/2], Step [155/938], Loss: 0.1140\n",
      "Epoch [2/2], Step [156/938], Loss: 0.1741\n",
      "Epoch [2/2], Step [157/938], Loss: 0.2478\n",
      "Epoch [2/2], Step [158/938], Loss: 0.1356\n",
      "Epoch [2/2], Step [159/938], Loss: 0.0602\n",
      "Epoch [2/2], Step [160/938], Loss: 0.3200\n",
      "Epoch [2/2], Step [161/938], Loss: 0.1946\n",
      "Epoch [2/2], Step [162/938], Loss: 0.1579\n",
      "Epoch [2/2], Step [163/938], Loss: 0.3252\n",
      "Epoch [2/2], Step [164/938], Loss: 0.0560\n",
      "Epoch [2/2], Step [165/938], Loss: 0.0460\n",
      "Epoch [2/2], Step [166/938], Loss: 0.0886\n",
      "Epoch [2/2], Step [167/938], Loss: 0.1877\n",
      "Epoch [2/2], Step [168/938], Loss: 0.0756\n",
      "Epoch [2/2], Step [169/938], Loss: 0.0960\n",
      "Epoch [2/2], Step [170/938], Loss: 0.0969\n",
      "Epoch [2/2], Step [171/938], Loss: 0.0620\n",
      "Epoch [2/2], Step [172/938], Loss: 0.2229\n",
      "Epoch [2/2], Step [173/938], Loss: 0.1301\n",
      "Epoch [2/2], Step [174/938], Loss: 0.0865\n",
      "Epoch [2/2], Step [175/938], Loss: 0.1272\n",
      "Epoch [2/2], Step [176/938], Loss: 0.1102\n",
      "Epoch [2/2], Step [177/938], Loss: 0.2941\n",
      "Epoch [2/2], Step [178/938], Loss: 0.1296\n",
      "Epoch [2/2], Step [179/938], Loss: 0.1696\n",
      "Epoch [2/2], Step [180/938], Loss: 0.0641\n",
      "Epoch [2/2], Step [181/938], Loss: 0.0759\n",
      "Epoch [2/2], Step [182/938], Loss: 0.0455\n",
      "Epoch [2/2], Step [183/938], Loss: 0.0614\n",
      "Epoch [2/2], Step [184/938], Loss: 0.1441\n",
      "Epoch [2/2], Step [185/938], Loss: 0.2649\n",
      "Epoch [2/2], Step [186/938], Loss: 0.3548\n",
      "Epoch [2/2], Step [187/938], Loss: 0.0387\n",
      "Epoch [2/2], Step [188/938], Loss: 0.1616\n",
      "Epoch [2/2], Step [189/938], Loss: 0.1641\n",
      "Epoch [2/2], Step [190/938], Loss: 0.4013\n",
      "Epoch [2/2], Step [191/938], Loss: 0.2792\n",
      "Epoch [2/2], Step [192/938], Loss: 0.1216\n",
      "Epoch [2/2], Step [193/938], Loss: 0.3008\n",
      "Epoch [2/2], Step [194/938], Loss: 0.2911\n",
      "Epoch [2/2], Step [195/938], Loss: 0.2832\n",
      "Epoch [2/2], Step [196/938], Loss: 0.0251\n",
      "Epoch [2/2], Step [197/938], Loss: 0.0341\n",
      "Epoch [2/2], Step [198/938], Loss: 0.0763\n",
      "Epoch [2/2], Step [199/938], Loss: 0.1357\n",
      "Epoch [2/2], Step [200/938], Loss: 0.0529\n",
      "Epoch [2/2], Step [201/938], Loss: 0.2620\n",
      "Epoch [2/2], Step [202/938], Loss: 0.1851\n",
      "Epoch [2/2], Step [203/938], Loss: 0.0904\n",
      "Epoch [2/2], Step [204/938], Loss: 0.0523\n",
      "Epoch [2/2], Step [205/938], Loss: 0.0483\n",
      "Epoch [2/2], Step [206/938], Loss: 0.0530\n",
      "Epoch [2/2], Step [207/938], Loss: 0.2371\n",
      "Epoch [2/2], Step [208/938], Loss: 0.0528\n",
      "Epoch [2/2], Step [209/938], Loss: 0.0561\n",
      "Epoch [2/2], Step [210/938], Loss: 0.3605\n",
      "Epoch [2/2], Step [211/938], Loss: 0.1747\n",
      "Epoch [2/2], Step [212/938], Loss: 0.1345\n",
      "Epoch [2/2], Step [213/938], Loss: 0.0830\n",
      "Epoch [2/2], Step [214/938], Loss: 0.1543\n",
      "Epoch [2/2], Step [215/938], Loss: 0.0614\n",
      "Epoch [2/2], Step [216/938], Loss: 0.0943\n",
      "Epoch [2/2], Step [217/938], Loss: 0.0797\n",
      "Epoch [2/2], Step [218/938], Loss: 0.1261\n",
      "Epoch [2/2], Step [219/938], Loss: 0.1544\n",
      "Epoch [2/2], Step [220/938], Loss: 0.1016\n",
      "Epoch [2/2], Step [221/938], Loss: 0.1164\n",
      "Epoch [2/2], Step [222/938], Loss: 0.0704\n",
      "Epoch [2/2], Step [223/938], Loss: 0.2030\n",
      "Epoch [2/2], Step [224/938], Loss: 0.1536\n",
      "Epoch [2/2], Step [225/938], Loss: 0.0809\n",
      "Epoch [2/2], Step [226/938], Loss: 0.0917\n",
      "Epoch [2/2], Step [227/938], Loss: 0.2468\n",
      "Epoch [2/2], Step [228/938], Loss: 0.1619\n",
      "Epoch [2/2], Step [229/938], Loss: 0.1737\n",
      "Epoch [2/2], Step [230/938], Loss: 0.1288\n",
      "Epoch [2/2], Step [231/938], Loss: 0.0801\n",
      "Epoch [2/2], Step [232/938], Loss: 0.1295\n",
      "Epoch [2/2], Step [233/938], Loss: 0.0853\n",
      "Epoch [2/2], Step [234/938], Loss: 0.1485\n",
      "Epoch [2/2], Step [235/938], Loss: 0.2041\n",
      "Epoch [2/2], Step [236/938], Loss: 0.1448\n",
      "Epoch [2/2], Step [237/938], Loss: 0.1942\n",
      "Epoch [2/2], Step [238/938], Loss: 0.3231\n",
      "Epoch [2/2], Step [239/938], Loss: 0.2168\n",
      "Epoch [2/2], Step [240/938], Loss: 0.1014\n",
      "Epoch [2/2], Step [241/938], Loss: 0.0792\n",
      "Epoch [2/2], Step [242/938], Loss: 0.1216\n",
      "Epoch [2/2], Step [243/938], Loss: 0.1120\n",
      "Epoch [2/2], Step [244/938], Loss: 0.0753\n",
      "Epoch [2/2], Step [245/938], Loss: 0.1280\n",
      "Epoch [2/2], Step [246/938], Loss: 0.1256\n",
      "Epoch [2/2], Step [247/938], Loss: 0.0362\n",
      "Epoch [2/2], Step [248/938], Loss: 0.2107\n",
      "Epoch [2/2], Step [249/938], Loss: 0.0350\n",
      "Epoch [2/2], Step [250/938], Loss: 0.0923\n",
      "Epoch [2/2], Step [251/938], Loss: 0.1713\n",
      "Epoch [2/2], Step [252/938], Loss: 0.0603\n",
      "Epoch [2/2], Step [253/938], Loss: 0.4983\n",
      "Epoch [2/2], Step [254/938], Loss: 0.1250\n",
      "Epoch [2/2], Step [255/938], Loss: 0.1952\n",
      "Epoch [2/2], Step [256/938], Loss: 0.0511\n",
      "Epoch [2/2], Step [257/938], Loss: 0.2789\n",
      "Epoch [2/2], Step [258/938], Loss: 0.1861\n",
      "Epoch [2/2], Step [259/938], Loss: 0.2595\n",
      "Epoch [2/2], Step [260/938], Loss: 0.0633\n",
      "Epoch [2/2], Step [261/938], Loss: 0.0563\n",
      "Epoch [2/2], Step [262/938], Loss: 0.1706\n",
      "Epoch [2/2], Step [263/938], Loss: 0.0838\n",
      "Epoch [2/2], Step [264/938], Loss: 0.0992\n",
      "Epoch [2/2], Step [265/938], Loss: 0.4796\n",
      "Epoch [2/2], Step [266/938], Loss: 0.4195\n",
      "Epoch [2/2], Step [267/938], Loss: 0.1312\n",
      "Epoch [2/2], Step [268/938], Loss: 0.1774\n",
      "Epoch [2/2], Step [269/938], Loss: 0.0649\n",
      "Epoch [2/2], Step [270/938], Loss: 0.2222\n",
      "Epoch [2/2], Step [271/938], Loss: 0.1341\n",
      "Epoch [2/2], Step [272/938], Loss: 0.0593\n",
      "Epoch [2/2], Step [273/938], Loss: 0.1221\n",
      "Epoch [2/2], Step [274/938], Loss: 0.1254\n",
      "Epoch [2/2], Step [275/938], Loss: 0.0431\n",
      "Epoch [2/2], Step [276/938], Loss: 0.0408\n",
      "Epoch [2/2], Step [277/938], Loss: 0.1102\n",
      "Epoch [2/2], Step [278/938], Loss: 0.1321\n",
      "Epoch [2/2], Step [279/938], Loss: 0.1103\n",
      "Epoch [2/2], Step [280/938], Loss: 0.1419\n",
      "Epoch [2/2], Step [281/938], Loss: 0.0460\n",
      "Epoch [2/2], Step [282/938], Loss: 0.1313\n",
      "Epoch [2/2], Step [283/938], Loss: 0.3658\n",
      "Epoch [2/2], Step [284/938], Loss: 0.0647\n",
      "Epoch [2/2], Step [285/938], Loss: 0.0815\n",
      "Epoch [2/2], Step [286/938], Loss: 0.1486\n",
      "Epoch [2/2], Step [287/938], Loss: 0.0227\n",
      "Epoch [2/2], Step [288/938], Loss: 0.2321\n",
      "Epoch [2/2], Step [289/938], Loss: 0.0993\n",
      "Epoch [2/2], Step [290/938], Loss: 0.0249\n",
      "Epoch [2/2], Step [291/938], Loss: 0.2816\n",
      "Epoch [2/2], Step [292/938], Loss: 0.2616\n",
      "Epoch [2/2], Step [293/938], Loss: 0.1850\n",
      "Epoch [2/2], Step [294/938], Loss: 0.1232\n",
      "Epoch [2/2], Step [295/938], Loss: 0.0437\n",
      "Epoch [2/2], Step [296/938], Loss: 0.0361\n",
      "Epoch [2/2], Step [297/938], Loss: 0.0661\n",
      "Epoch [2/2], Step [298/938], Loss: 0.1131\n",
      "Epoch [2/2], Step [299/938], Loss: 0.0400\n",
      "Epoch [2/2], Step [300/938], Loss: 0.0113\n",
      "Epoch [2/2], Step [301/938], Loss: 0.1002\n",
      "Epoch [2/2], Step [302/938], Loss: 0.1890\n",
      "Epoch [2/2], Step [303/938], Loss: 0.0787\n",
      "Epoch [2/2], Step [304/938], Loss: 0.0815\n",
      "Epoch [2/2], Step [305/938], Loss: 0.0214\n",
      "Epoch [2/2], Step [306/938], Loss: 0.0924\n",
      "Epoch [2/2], Step [307/938], Loss: 0.0060\n",
      "Epoch [2/2], Step [308/938], Loss: 0.1064\n",
      "Epoch [2/2], Step [309/938], Loss: 0.1224\n",
      "Epoch [2/2], Step [310/938], Loss: 0.0372\n",
      "Epoch [2/2], Step [311/938], Loss: 0.0576\n",
      "Epoch [2/2], Step [312/938], Loss: 0.1209\n",
      "Epoch [2/2], Step [313/938], Loss: 0.2437\n",
      "Epoch [2/2], Step [314/938], Loss: 0.0362\n",
      "Epoch [2/2], Step [315/938], Loss: 0.0974\n",
      "Epoch [2/2], Step [316/938], Loss: 0.0430\n",
      "Epoch [2/2], Step [317/938], Loss: 0.0900\n",
      "Epoch [2/2], Step [318/938], Loss: 0.0443\n",
      "Epoch [2/2], Step [319/938], Loss: 0.1519\n",
      "Epoch [2/2], Step [320/938], Loss: 0.0738\n",
      "Epoch [2/2], Step [321/938], Loss: 0.0870\n",
      "Epoch [2/2], Step [322/938], Loss: 0.2716\n",
      "Epoch [2/2], Step [323/938], Loss: 0.2116\n",
      "Epoch [2/2], Step [324/938], Loss: 0.3490\n",
      "Epoch [2/2], Step [325/938], Loss: 0.0241\n",
      "Epoch [2/2], Step [326/938], Loss: 0.2263\n",
      "Epoch [2/2], Step [327/938], Loss: 0.1333\n",
      "Epoch [2/2], Step [328/938], Loss: 0.0850\n",
      "Epoch [2/2], Step [329/938], Loss: 0.2709\n",
      "Epoch [2/2], Step [330/938], Loss: 0.1167\n",
      "Epoch [2/2], Step [331/938], Loss: 0.3268\n",
      "Epoch [2/2], Step [332/938], Loss: 0.1323\n",
      "Epoch [2/2], Step [333/938], Loss: 0.0957\n",
      "Epoch [2/2], Step [334/938], Loss: 0.1098\n",
      "Epoch [2/2], Step [335/938], Loss: 0.1371\n",
      "Epoch [2/2], Step [336/938], Loss: 0.3626\n",
      "Epoch [2/2], Step [337/938], Loss: 0.0955\n",
      "Epoch [2/2], Step [338/938], Loss: 0.0166\n",
      "Epoch [2/2], Step [339/938], Loss: 0.3485\n",
      "Epoch [2/2], Step [340/938], Loss: 0.0867\n",
      "Epoch [2/2], Step [341/938], Loss: 0.2389\n",
      "Epoch [2/2], Step [342/938], Loss: 0.2125\n",
      "Epoch [2/2], Step [343/938], Loss: 0.0693\n",
      "Epoch [2/2], Step [344/938], Loss: 0.1634\n",
      "Epoch [2/2], Step [345/938], Loss: 0.0945\n",
      "Epoch [2/2], Step [346/938], Loss: 0.1011\n",
      "Epoch [2/2], Step [347/938], Loss: 0.0848\n",
      "Epoch [2/2], Step [348/938], Loss: 0.2193\n",
      "Epoch [2/2], Step [349/938], Loss: 0.0456\n",
      "Epoch [2/2], Step [350/938], Loss: 0.1394\n",
      "Epoch [2/2], Step [351/938], Loss: 0.2116\n",
      "Epoch [2/2], Step [352/938], Loss: 0.1770\n",
      "Epoch [2/2], Step [353/938], Loss: 0.1224\n",
      "Epoch [2/2], Step [354/938], Loss: 0.2878\n",
      "Epoch [2/2], Step [355/938], Loss: 0.1046\n",
      "Epoch [2/2], Step [356/938], Loss: 0.0688\n",
      "Epoch [2/2], Step [357/938], Loss: 0.0323\n",
      "Epoch [2/2], Step [358/938], Loss: 0.1998\n",
      "Epoch [2/2], Step [359/938], Loss: 0.0294\n",
      "Epoch [2/2], Step [360/938], Loss: 0.2666\n",
      "Epoch [2/2], Step [361/938], Loss: 0.0747\n",
      "Epoch [2/2], Step [362/938], Loss: 0.1650\n",
      "Epoch [2/2], Step [363/938], Loss: 0.1067\n",
      "Epoch [2/2], Step [364/938], Loss: 0.2061\n",
      "Epoch [2/2], Step [365/938], Loss: 0.3572\n",
      "Epoch [2/2], Step [366/938], Loss: 0.3039\n",
      "Epoch [2/2], Step [367/938], Loss: 0.1959\n",
      "Epoch [2/2], Step [368/938], Loss: 0.1249\n",
      "Epoch [2/2], Step [369/938], Loss: 0.1611\n",
      "Epoch [2/2], Step [370/938], Loss: 0.0941\n",
      "Epoch [2/2], Step [371/938], Loss: 0.0809\n",
      "Epoch [2/2], Step [372/938], Loss: 0.0728\n",
      "Epoch [2/2], Step [373/938], Loss: 0.1046\n",
      "Epoch [2/2], Step [374/938], Loss: 0.0566\n",
      "Epoch [2/2], Step [375/938], Loss: 0.1346\n",
      "Epoch [2/2], Step [376/938], Loss: 0.0358\n",
      "Epoch [2/2], Step [377/938], Loss: 0.0520\n",
      "Epoch [2/2], Step [378/938], Loss: 0.1268\n",
      "Epoch [2/2], Step [379/938], Loss: 0.0864\n",
      "Epoch [2/2], Step [380/938], Loss: 0.0288\n",
      "Epoch [2/2], Step [381/938], Loss: 0.0914\n",
      "Epoch [2/2], Step [382/938], Loss: 0.0507\n",
      "Epoch [2/2], Step [383/938], Loss: 0.0755\n",
      "Epoch [2/2], Step [384/938], Loss: 0.0926\n",
      "Epoch [2/2], Step [385/938], Loss: 0.0181\n",
      "Epoch [2/2], Step [386/938], Loss: 0.0972\n",
      "Epoch [2/2], Step [387/938], Loss: 0.1789\n",
      "Epoch [2/2], Step [388/938], Loss: 0.1484\n",
      "Epoch [2/2], Step [389/938], Loss: 0.0987\n",
      "Epoch [2/2], Step [390/938], Loss: 0.0801\n",
      "Epoch [2/2], Step [391/938], Loss: 0.0468\n",
      "Epoch [2/2], Step [392/938], Loss: 0.0295\n",
      "Epoch [2/2], Step [393/938], Loss: 0.1128\n",
      "Epoch [2/2], Step [394/938], Loss: 0.2232\n",
      "Epoch [2/2], Step [395/938], Loss: 0.2837\n",
      "Epoch [2/2], Step [396/938], Loss: 0.0625\n",
      "Epoch [2/2], Step [397/938], Loss: 0.1838\n",
      "Epoch [2/2], Step [398/938], Loss: 0.0564\n",
      "Epoch [2/2], Step [399/938], Loss: 0.2488\n",
      "Epoch [2/2], Step [400/938], Loss: 0.3200\n",
      "Epoch [2/2], Step [401/938], Loss: 0.1459\n",
      "Epoch [2/2], Step [402/938], Loss: 0.3435\n",
      "Epoch [2/2], Step [403/938], Loss: 0.2466\n",
      "Epoch [2/2], Step [404/938], Loss: 0.1231\n",
      "Epoch [2/2], Step [405/938], Loss: 0.1183\n",
      "Epoch [2/2], Step [406/938], Loss: 0.0694\n",
      "Epoch [2/2], Step [407/938], Loss: 0.2769\n",
      "Epoch [2/2], Step [408/938], Loss: 0.1018\n",
      "Epoch [2/2], Step [409/938], Loss: 0.0728\n",
      "Epoch [2/2], Step [410/938], Loss: 0.2063\n",
      "Epoch [2/2], Step [411/938], Loss: 0.0887\n",
      "Epoch [2/2], Step [412/938], Loss: 0.0537\n",
      "Epoch [2/2], Step [413/938], Loss: 0.0084\n",
      "Epoch [2/2], Step [414/938], Loss: 0.1295\n",
      "Epoch [2/2], Step [415/938], Loss: 0.0865\n",
      "Epoch [2/2], Step [416/938], Loss: 0.1759\n",
      "Epoch [2/2], Step [417/938], Loss: 0.3249\n",
      "Epoch [2/2], Step [418/938], Loss: 0.1786\n",
      "Epoch [2/2], Step [419/938], Loss: 0.0997\n",
      "Epoch [2/2], Step [420/938], Loss: 0.2177\n",
      "Epoch [2/2], Step [421/938], Loss: 0.2383\n",
      "Epoch [2/2], Step [422/938], Loss: 0.0979\n",
      "Epoch [2/2], Step [423/938], Loss: 0.1090\n",
      "Epoch [2/2], Step [424/938], Loss: 0.0382\n",
      "Epoch [2/2], Step [425/938], Loss: 0.1152\n",
      "Epoch [2/2], Step [426/938], Loss: 0.0464\n",
      "Epoch [2/2], Step [427/938], Loss: 0.1068\n",
      "Epoch [2/2], Step [428/938], Loss: 0.0140\n",
      "Epoch [2/2], Step [429/938], Loss: 0.1167\n",
      "Epoch [2/2], Step [430/938], Loss: 0.1220\n",
      "Epoch [2/2], Step [431/938], Loss: 0.0897\n",
      "Epoch [2/2], Step [432/938], Loss: 0.2027\n",
      "Epoch [2/2], Step [433/938], Loss: 0.0343\n",
      "Epoch [2/2], Step [434/938], Loss: 0.0779\n",
      "Epoch [2/2], Step [435/938], Loss: 0.1128\n",
      "Epoch [2/2], Step [436/938], Loss: 0.1637\n",
      "Epoch [2/2], Step [437/938], Loss: 0.0738\n",
      "Epoch [2/2], Step [438/938], Loss: 0.2314\n",
      "Epoch [2/2], Step [439/938], Loss: 0.1097\n",
      "Epoch [2/2], Step [440/938], Loss: 0.1859\n",
      "Epoch [2/2], Step [441/938], Loss: 0.1384\n",
      "Epoch [2/2], Step [442/938], Loss: 0.0379\n",
      "Epoch [2/2], Step [443/938], Loss: 0.0613\n",
      "Epoch [2/2], Step [444/938], Loss: 0.2573\n",
      "Epoch [2/2], Step [445/938], Loss: 0.0449\n",
      "Epoch [2/2], Step [446/938], Loss: 0.2209\n",
      "Epoch [2/2], Step [447/938], Loss: 0.0937\n",
      "Epoch [2/2], Step [448/938], Loss: 0.1260\n",
      "Epoch [2/2], Step [449/938], Loss: 0.2005\n",
      "Epoch [2/2], Step [450/938], Loss: 0.1228\n",
      "Epoch [2/2], Step [451/938], Loss: 0.1183\n",
      "Epoch [2/2], Step [452/938], Loss: 0.1630\n",
      "Epoch [2/2], Step [453/938], Loss: 0.2366\n",
      "Epoch [2/2], Step [454/938], Loss: 0.0337\n",
      "Epoch [2/2], Step [455/938], Loss: 0.1076\n",
      "Epoch [2/2], Step [456/938], Loss: 0.0900\n",
      "Epoch [2/2], Step [457/938], Loss: 0.0584\n",
      "Epoch [2/2], Step [458/938], Loss: 0.0616\n",
      "Epoch [2/2], Step [459/938], Loss: 0.2476\n",
      "Epoch [2/2], Step [460/938], Loss: 0.0795\n",
      "Epoch [2/2], Step [461/938], Loss: 0.3335\n",
      "Epoch [2/2], Step [462/938], Loss: 0.0866\n",
      "Epoch [2/2], Step [463/938], Loss: 0.0478\n",
      "Epoch [2/2], Step [464/938], Loss: 0.0865\n",
      "Epoch [2/2], Step [465/938], Loss: 0.0167\n",
      "Epoch [2/2], Step [466/938], Loss: 0.2607\n",
      "Epoch [2/2], Step [467/938], Loss: 0.1916\n",
      "Epoch [2/2], Step [468/938], Loss: 0.0919\n",
      "Epoch [2/2], Step [469/938], Loss: 0.1431\n",
      "Epoch [2/2], Step [470/938], Loss: 0.1504\n",
      "Epoch [2/2], Step [471/938], Loss: 0.0446\n",
      "Epoch [2/2], Step [472/938], Loss: 0.3175\n",
      "Epoch [2/2], Step [473/938], Loss: 0.2104\n",
      "Epoch [2/2], Step [474/938], Loss: 0.1459\n",
      "Epoch [2/2], Step [475/938], Loss: 0.2097\n",
      "Epoch [2/2], Step [476/938], Loss: 0.0855\n",
      "Epoch [2/2], Step [477/938], Loss: 0.2116\n",
      "Epoch [2/2], Step [478/938], Loss: 0.0133\n",
      "Epoch [2/2], Step [479/938], Loss: 0.0978\n",
      "Epoch [2/2], Step [480/938], Loss: 0.0716\n",
      "Epoch [2/2], Step [481/938], Loss: 0.2149\n",
      "Epoch [2/2], Step [482/938], Loss: 0.0811\n",
      "Epoch [2/2], Step [483/938], Loss: 0.0801\n",
      "Epoch [2/2], Step [484/938], Loss: 0.2768\n",
      "Epoch [2/2], Step [485/938], Loss: 0.1245\n",
      "Epoch [2/2], Step [486/938], Loss: 0.0259\n",
      "Epoch [2/2], Step [487/938], Loss: 0.2798\n",
      "Epoch [2/2], Step [488/938], Loss: 0.0556\n",
      "Epoch [2/2], Step [489/938], Loss: 0.0314\n",
      "Epoch [2/2], Step [490/938], Loss: 0.2684\n",
      "Epoch [2/2], Step [491/938], Loss: 0.2389\n",
      "Epoch [2/2], Step [492/938], Loss: 0.2430\n",
      "Epoch [2/2], Step [493/938], Loss: 0.2304\n",
      "Epoch [2/2], Step [494/938], Loss: 0.1144\n",
      "Epoch [2/2], Step [495/938], Loss: 0.0583\n",
      "Epoch [2/2], Step [496/938], Loss: 0.0999\n",
      "Epoch [2/2], Step [497/938], Loss: 0.0358\n",
      "Epoch [2/2], Step [498/938], Loss: 0.1041\n",
      "Epoch [2/2], Step [499/938], Loss: 0.1282\n",
      "Epoch [2/2], Step [500/938], Loss: 0.4079\n",
      "Epoch [2/2], Step [501/938], Loss: 0.0884\n",
      "Epoch [2/2], Step [502/938], Loss: 0.0440\n",
      "Epoch [2/2], Step [503/938], Loss: 0.0401\n",
      "Epoch [2/2], Step [504/938], Loss: 0.0762\n",
      "Epoch [2/2], Step [505/938], Loss: 0.0484\n",
      "Epoch [2/2], Step [506/938], Loss: 0.1535\n",
      "Epoch [2/2], Step [507/938], Loss: 0.1152\n",
      "Epoch [2/2], Step [508/938], Loss: 0.0919\n",
      "Epoch [2/2], Step [509/938], Loss: 0.1268\n",
      "Epoch [2/2], Step [510/938], Loss: 0.0616\n",
      "Epoch [2/2], Step [511/938], Loss: 0.0340\n",
      "Epoch [2/2], Step [512/938], Loss: 0.0907\n",
      "Epoch [2/2], Step [513/938], Loss: 0.0224\n",
      "Epoch [2/2], Step [514/938], Loss: 0.1325\n",
      "Epoch [2/2], Step [515/938], Loss: 0.0164\n",
      "Epoch [2/2], Step [516/938], Loss: 0.1183\n",
      "Epoch [2/2], Step [517/938], Loss: 0.2160\n",
      "Epoch [2/2], Step [518/938], Loss: 0.0653\n",
      "Epoch [2/2], Step [519/938], Loss: 0.0262\n",
      "Epoch [2/2], Step [520/938], Loss: 0.0253\n",
      "Epoch [2/2], Step [521/938], Loss: 0.3186\n",
      "Epoch [2/2], Step [522/938], Loss: 0.1319\n",
      "Epoch [2/2], Step [523/938], Loss: 0.0979\n",
      "Epoch [2/2], Step [524/938], Loss: 0.1661\n",
      "Epoch [2/2], Step [525/938], Loss: 0.1254\n",
      "Epoch [2/2], Step [526/938], Loss: 0.1620\n",
      "Epoch [2/2], Step [527/938], Loss: 0.1870\n",
      "Epoch [2/2], Step [528/938], Loss: 0.1416\n",
      "Epoch [2/2], Step [529/938], Loss: 0.0308\n",
      "Epoch [2/2], Step [530/938], Loss: 0.2155\n",
      "Epoch [2/2], Step [531/938], Loss: 0.0401\n",
      "Epoch [2/2], Step [532/938], Loss: 0.0323\n",
      "Epoch [2/2], Step [533/938], Loss: 0.0979\n",
      "Epoch [2/2], Step [534/938], Loss: 0.0835\n",
      "Epoch [2/2], Step [535/938], Loss: 0.0388\n",
      "Epoch [2/2], Step [536/938], Loss: 0.1762\n",
      "Epoch [2/2], Step [537/938], Loss: 0.0486\n",
      "Epoch [2/2], Step [538/938], Loss: 0.0946\n",
      "Epoch [2/2], Step [539/938], Loss: 0.0558\n",
      "Epoch [2/2], Step [540/938], Loss: 0.1064\n",
      "Epoch [2/2], Step [541/938], Loss: 0.1695\n",
      "Epoch [2/2], Step [542/938], Loss: 0.0935\n",
      "Epoch [2/2], Step [543/938], Loss: 0.1226\n",
      "Epoch [2/2], Step [544/938], Loss: 0.1381\n",
      "Epoch [2/2], Step [545/938], Loss: 0.2267\n",
      "Epoch [2/2], Step [546/938], Loss: 0.1828\n",
      "Epoch [2/2], Step [547/938], Loss: 0.0793\n",
      "Epoch [2/2], Step [548/938], Loss: 0.0162\n",
      "Epoch [2/2], Step [549/938], Loss: 0.1586\n",
      "Epoch [2/2], Step [550/938], Loss: 0.1597\n",
      "Epoch [2/2], Step [551/938], Loss: 0.3385\n",
      "Epoch [2/2], Step [552/938], Loss: 0.1355\n",
      "Epoch [2/2], Step [553/938], Loss: 0.0508\n",
      "Epoch [2/2], Step [554/938], Loss: 0.1964\n",
      "Epoch [2/2], Step [555/938], Loss: 0.2981\n",
      "Epoch [2/2], Step [556/938], Loss: 0.0268\n",
      "Epoch [2/2], Step [557/938], Loss: 0.0653\n",
      "Epoch [2/2], Step [558/938], Loss: 0.2395\n",
      "Epoch [2/2], Step [559/938], Loss: 0.2313\n",
      "Epoch [2/2], Step [560/938], Loss: 0.1020\n",
      "Epoch [2/2], Step [561/938], Loss: 0.1188\n",
      "Epoch [2/2], Step [562/938], Loss: 0.0418\n",
      "Epoch [2/2], Step [563/938], Loss: 0.0824\n",
      "Epoch [2/2], Step [564/938], Loss: 0.0785\n",
      "Epoch [2/2], Step [565/938], Loss: 0.3197\n",
      "Epoch [2/2], Step [566/938], Loss: 0.3714\n",
      "Epoch [2/2], Step [567/938], Loss: 0.0624\n",
      "Epoch [2/2], Step [568/938], Loss: 0.1898\n",
      "Epoch [2/2], Step [569/938], Loss: 0.1689\n",
      "Epoch [2/2], Step [570/938], Loss: 0.1908\n",
      "Epoch [2/2], Step [571/938], Loss: 0.3284\n",
      "Epoch [2/2], Step [572/938], Loss: 0.1385\n",
      "Epoch [2/2], Step [573/938], Loss: 0.1707\n",
      "Epoch [2/2], Step [574/938], Loss: 0.2645\n",
      "Epoch [2/2], Step [575/938], Loss: 0.1886\n",
      "Epoch [2/2], Step [576/938], Loss: 0.1607\n",
      "Epoch [2/2], Step [577/938], Loss: 0.1268\n",
      "Epoch [2/2], Step [578/938], Loss: 0.1101\n",
      "Epoch [2/2], Step [579/938], Loss: 0.1669\n",
      "Epoch [2/2], Step [580/938], Loss: 0.2751\n",
      "Epoch [2/2], Step [581/938], Loss: 0.2266\n",
      "Epoch [2/2], Step [582/938], Loss: 0.0883\n",
      "Epoch [2/2], Step [583/938], Loss: 0.1286\n",
      "Epoch [2/2], Step [584/938], Loss: 0.1312\n",
      "Epoch [2/2], Step [585/938], Loss: 0.1141\n",
      "Epoch [2/2], Step [586/938], Loss: 0.1616\n",
      "Epoch [2/2], Step [587/938], Loss: 0.1333\n",
      "Epoch [2/2], Step [588/938], Loss: 0.1519\n",
      "Epoch [2/2], Step [589/938], Loss: 0.1927\n",
      "Epoch [2/2], Step [590/938], Loss: 0.2295\n",
      "Epoch [2/2], Step [591/938], Loss: 0.2584\n",
      "Epoch [2/2], Step [592/938], Loss: 0.1939\n",
      "Epoch [2/2], Step [593/938], Loss: 0.0391\n",
      "Epoch [2/2], Step [594/938], Loss: 0.1269\n",
      "Epoch [2/2], Step [595/938], Loss: 0.2044\n",
      "Epoch [2/2], Step [596/938], Loss: 0.0246\n",
      "Epoch [2/2], Step [597/938], Loss: 0.2961\n",
      "Epoch [2/2], Step [598/938], Loss: 0.3663\n",
      "Epoch [2/2], Step [599/938], Loss: 0.1605\n",
      "Epoch [2/2], Step [600/938], Loss: 0.0931\n",
      "Epoch [2/2], Step [601/938], Loss: 0.1300\n",
      "Epoch [2/2], Step [602/938], Loss: 0.1831\n",
      "Epoch [2/2], Step [603/938], Loss: 0.0346\n",
      "Epoch [2/2], Step [604/938], Loss: 0.0529\n",
      "Epoch [2/2], Step [605/938], Loss: 0.1278\n",
      "Epoch [2/2], Step [606/938], Loss: 0.1089\n",
      "Epoch [2/2], Step [607/938], Loss: 0.0536\n",
      "Epoch [2/2], Step [608/938], Loss: 0.0820\n",
      "Epoch [2/2], Step [609/938], Loss: 0.0568\n",
      "Epoch [2/2], Step [610/938], Loss: 0.2501\n",
      "Epoch [2/2], Step [611/938], Loss: 0.3022\n",
      "Epoch [2/2], Step [612/938], Loss: 0.1420\n",
      "Epoch [2/2], Step [613/938], Loss: 0.0684\n",
      "Epoch [2/2], Step [614/938], Loss: 0.1337\n",
      "Epoch [2/2], Step [615/938], Loss: 0.0616\n",
      "Epoch [2/2], Step [616/938], Loss: 0.0428\n",
      "Epoch [2/2], Step [617/938], Loss: 0.0857\n",
      "Epoch [2/2], Step [618/938], Loss: 0.0877\n",
      "Epoch [2/2], Step [619/938], Loss: 0.0657\n",
      "Epoch [2/2], Step [620/938], Loss: 0.0292\n",
      "Epoch [2/2], Step [621/938], Loss: 0.1040\n",
      "Epoch [2/2], Step [622/938], Loss: 0.0706\n",
      "Epoch [2/2], Step [623/938], Loss: 0.1315\n",
      "Epoch [2/2], Step [624/938], Loss: 0.0531\n",
      "Epoch [2/2], Step [625/938], Loss: 0.0555\n",
      "Epoch [2/2], Step [626/938], Loss: 0.1380\n",
      "Epoch [2/2], Step [627/938], Loss: 0.1163\n",
      "Epoch [2/2], Step [628/938], Loss: 0.1185\n",
      "Epoch [2/2], Step [629/938], Loss: 0.0237\n",
      "Epoch [2/2], Step [630/938], Loss: 0.1114\n",
      "Epoch [2/2], Step [631/938], Loss: 0.2219\n",
      "Epoch [2/2], Step [632/938], Loss: 0.0347\n",
      "Epoch [2/2], Step [633/938], Loss: 0.1207\n",
      "Epoch [2/2], Step [634/938], Loss: 0.0277\n",
      "Epoch [2/2], Step [635/938], Loss: 0.1347\n",
      "Epoch [2/2], Step [636/938], Loss: 0.0576\n",
      "Epoch [2/2], Step [637/938], Loss: 0.1356\n",
      "Epoch [2/2], Step [638/938], Loss: 0.0586\n",
      "Epoch [2/2], Step [639/938], Loss: 0.0712\n",
      "Epoch [2/2], Step [640/938], Loss: 0.1113\n",
      "Epoch [2/2], Step [641/938], Loss: 0.1531\n",
      "Epoch [2/2], Step [642/938], Loss: 0.0560\n",
      "Epoch [2/2], Step [643/938], Loss: 0.3031\n",
      "Epoch [2/2], Step [644/938], Loss: 0.2177\n",
      "Epoch [2/2], Step [645/938], Loss: 0.0123\n",
      "Epoch [2/2], Step [646/938], Loss: 0.0953\n",
      "Epoch [2/2], Step [647/938], Loss: 0.0677\n",
      "Epoch [2/2], Step [648/938], Loss: 0.0657\n",
      "Epoch [2/2], Step [649/938], Loss: 0.4115\n",
      "Epoch [2/2], Step [650/938], Loss: 0.2075\n",
      "Epoch [2/2], Step [651/938], Loss: 0.1374\n",
      "Epoch [2/2], Step [652/938], Loss: 0.4627\n",
      "Epoch [2/2], Step [653/938], Loss: 0.1203\n",
      "Epoch [2/2], Step [654/938], Loss: 0.1623\n",
      "Epoch [2/2], Step [655/938], Loss: 0.1650\n",
      "Epoch [2/2], Step [656/938], Loss: 0.0719\n",
      "Epoch [2/2], Step [657/938], Loss: 0.2186\n",
      "Epoch [2/2], Step [658/938], Loss: 0.0665\n",
      "Epoch [2/2], Step [659/938], Loss: 0.1046\n",
      "Epoch [2/2], Step [660/938], Loss: 0.0567\n",
      "Epoch [2/2], Step [661/938], Loss: 0.2087\n",
      "Epoch [2/2], Step [662/938], Loss: 0.1219\n",
      "Epoch [2/2], Step [663/938], Loss: 0.0196\n",
      "Epoch [2/2], Step [664/938], Loss: 0.0283\n",
      "Epoch [2/2], Step [665/938], Loss: 0.1339\n",
      "Epoch [2/2], Step [666/938], Loss: 0.1308\n",
      "Epoch [2/2], Step [667/938], Loss: 0.0514\n",
      "Epoch [2/2], Step [668/938], Loss: 0.0541\n",
      "Epoch [2/2], Step [669/938], Loss: 0.2108\n",
      "Epoch [2/2], Step [670/938], Loss: 0.1023\n",
      "Epoch [2/2], Step [671/938], Loss: 0.1178\n",
      "Epoch [2/2], Step [672/938], Loss: 0.0303\n",
      "Epoch [2/2], Step [673/938], Loss: 0.0723\n",
      "Epoch [2/2], Step [674/938], Loss: 0.0730\n",
      "Epoch [2/2], Step [675/938], Loss: 0.1229\n",
      "Epoch [2/2], Step [676/938], Loss: 0.2568\n",
      "Epoch [2/2], Step [677/938], Loss: 0.1254\n",
      "Epoch [2/2], Step [678/938], Loss: 0.1425\n",
      "Epoch [2/2], Step [679/938], Loss: 0.1571\n",
      "Epoch [2/2], Step [680/938], Loss: 0.1469\n",
      "Epoch [2/2], Step [681/938], Loss: 0.1940\n",
      "Epoch [2/2], Step [682/938], Loss: 0.2838\n",
      "Epoch [2/2], Step [683/938], Loss: 0.0339\n",
      "Epoch [2/2], Step [684/938], Loss: 0.0360\n",
      "Epoch [2/2], Step [685/938], Loss: 0.0140\n",
      "Epoch [2/2], Step [686/938], Loss: 0.0376\n",
      "Epoch [2/2], Step [687/938], Loss: 0.1070\n",
      "Epoch [2/2], Step [688/938], Loss: 0.1735\n",
      "Epoch [2/2], Step [689/938], Loss: 0.1269\n",
      "Epoch [2/2], Step [690/938], Loss: 0.0591\n",
      "Epoch [2/2], Step [691/938], Loss: 0.0359\n",
      "Epoch [2/2], Step [692/938], Loss: 0.0617\n",
      "Epoch [2/2], Step [693/938], Loss: 0.1567\n",
      "Epoch [2/2], Step [694/938], Loss: 0.2132\n",
      "Epoch [2/2], Step [695/938], Loss: 0.0995\n",
      "Epoch [2/2], Step [696/938], Loss: 0.1516\n",
      "Epoch [2/2], Step [697/938], Loss: 0.0304\n",
      "Epoch [2/2], Step [698/938], Loss: 0.0988\n",
      "Epoch [2/2], Step [699/938], Loss: 0.0167\n",
      "Epoch [2/2], Step [700/938], Loss: 0.1898\n",
      "Epoch [2/2], Step [701/938], Loss: 0.1201\n",
      "Epoch [2/2], Step [702/938], Loss: 0.2858\n",
      "Epoch [2/2], Step [703/938], Loss: 0.1193\n",
      "Epoch [2/2], Step [704/938], Loss: 0.1457\n",
      "Epoch [2/2], Step [705/938], Loss: 0.1135\n",
      "Epoch [2/2], Step [706/938], Loss: 0.0706\n",
      "Epoch [2/2], Step [707/938], Loss: 0.0794\n",
      "Epoch [2/2], Step [708/938], Loss: 0.0468\n",
      "Epoch [2/2], Step [709/938], Loss: 0.0177\n",
      "Epoch [2/2], Step [710/938], Loss: 0.0879\n",
      "Epoch [2/2], Step [711/938], Loss: 0.0561\n",
      "Epoch [2/2], Step [712/938], Loss: 0.0109\n",
      "Epoch [2/2], Step [713/938], Loss: 0.0873\n",
      "Epoch [2/2], Step [714/938], Loss: 0.1434\n",
      "Epoch [2/2], Step [715/938], Loss: 0.0813\n",
      "Epoch [2/2], Step [716/938], Loss: 0.1017\n",
      "Epoch [2/2], Step [717/938], Loss: 0.0247\n",
      "Epoch [2/2], Step [718/938], Loss: 0.0698\n",
      "Epoch [2/2], Step [719/938], Loss: 0.2877\n",
      "Epoch [2/2], Step [720/938], Loss: 0.1134\n",
      "Epoch [2/2], Step [721/938], Loss: 0.1131\n",
      "Epoch [2/2], Step [722/938], Loss: 0.0751\n",
      "Epoch [2/2], Step [723/938], Loss: 0.2224\n",
      "Epoch [2/2], Step [724/938], Loss: 0.2165\n",
      "Epoch [2/2], Step [725/938], Loss: 0.2665\n",
      "Epoch [2/2], Step [726/938], Loss: 0.1259\n",
      "Epoch [2/2], Step [727/938], Loss: 0.1195\n",
      "Epoch [2/2], Step [728/938], Loss: 0.1955\n",
      "Epoch [2/2], Step [729/938], Loss: 0.3760\n",
      "Epoch [2/2], Step [730/938], Loss: 0.0879\n",
      "Epoch [2/2], Step [731/938], Loss: 0.0514\n",
      "Epoch [2/2], Step [732/938], Loss: 0.0885\n",
      "Epoch [2/2], Step [733/938], Loss: 0.1005\n",
      "Epoch [2/2], Step [734/938], Loss: 0.0420\n",
      "Epoch [2/2], Step [735/938], Loss: 0.0606\n",
      "Epoch [2/2], Step [736/938], Loss: 0.1470\n",
      "Epoch [2/2], Step [737/938], Loss: 0.1338\n",
      "Epoch [2/2], Step [738/938], Loss: 0.1256\n",
      "Epoch [2/2], Step [739/938], Loss: 0.1446\n",
      "Epoch [2/2], Step [740/938], Loss: 0.2699\n",
      "Epoch [2/2], Step [741/938], Loss: 0.0657\n",
      "Epoch [2/2], Step [742/938], Loss: 0.2742\n",
      "Epoch [2/2], Step [743/938], Loss: 0.2425\n",
      "Epoch [2/2], Step [744/938], Loss: 0.0874\n",
      "Epoch [2/2], Step [745/938], Loss: 0.1708\n",
      "Epoch [2/2], Step [746/938], Loss: 0.0232\n",
      "Epoch [2/2], Step [747/938], Loss: 0.1529\n",
      "Epoch [2/2], Step [748/938], Loss: 0.0731\n",
      "Epoch [2/2], Step [749/938], Loss: 0.0722\n",
      "Epoch [2/2], Step [750/938], Loss: 0.1937\n",
      "Epoch [2/2], Step [751/938], Loss: 0.1232\n",
      "Epoch [2/2], Step [752/938], Loss: 0.0572\n",
      "Epoch [2/2], Step [753/938], Loss: 0.2170\n",
      "Epoch [2/2], Step [754/938], Loss: 0.0917\n",
      "Epoch [2/2], Step [755/938], Loss: 0.0385\n",
      "Epoch [2/2], Step [756/938], Loss: 0.1127\n",
      "Epoch [2/2], Step [757/938], Loss: 0.0129\n",
      "Epoch [2/2], Step [758/938], Loss: 0.0841\n",
      "Epoch [2/2], Step [759/938], Loss: 0.1643\n",
      "Epoch [2/2], Step [760/938], Loss: 0.1366\n",
      "Epoch [2/2], Step [761/938], Loss: 0.0780\n",
      "Epoch [2/2], Step [762/938], Loss: 0.0409\n",
      "Epoch [2/2], Step [763/938], Loss: 0.1404\n",
      "Epoch [2/2], Step [764/938], Loss: 0.0992\n",
      "Epoch [2/2], Step [765/938], Loss: 0.1875\n",
      "Epoch [2/2], Step [766/938], Loss: 0.0898\n",
      "Epoch [2/2], Step [767/938], Loss: 0.0595\n",
      "Epoch [2/2], Step [768/938], Loss: 0.3233\n",
      "Epoch [2/2], Step [769/938], Loss: 0.0521\n",
      "Epoch [2/2], Step [770/938], Loss: 0.2127\n",
      "Epoch [2/2], Step [771/938], Loss: 0.0404\n",
      "Epoch [2/2], Step [772/938], Loss: 0.0606\n",
      "Epoch [2/2], Step [773/938], Loss: 0.0668\n",
      "Epoch [2/2], Step [774/938], Loss: 0.1985\n",
      "Epoch [2/2], Step [775/938], Loss: 0.1872\n",
      "Epoch [2/2], Step [776/938], Loss: 0.1644\n",
      "Epoch [2/2], Step [777/938], Loss: 0.1276\n",
      "Epoch [2/2], Step [778/938], Loss: 0.0528\n",
      "Epoch [2/2], Step [779/938], Loss: 0.0583\n",
      "Epoch [2/2], Step [780/938], Loss: 0.1223\n",
      "Epoch [2/2], Step [781/938], Loss: 0.1418\n",
      "Epoch [2/2], Step [782/938], Loss: 0.0556\n",
      "Epoch [2/2], Step [783/938], Loss: 0.2597\n",
      "Epoch [2/2], Step [784/938], Loss: 0.1066\n",
      "Epoch [2/2], Step [785/938], Loss: 0.3020\n",
      "Epoch [2/2], Step [786/938], Loss: 0.0254\n",
      "Epoch [2/2], Step [787/938], Loss: 0.2268\n",
      "Epoch [2/2], Step [788/938], Loss: 0.0584\n",
      "Epoch [2/2], Step [789/938], Loss: 0.0428\n",
      "Epoch [2/2], Step [790/938], Loss: 0.1572\n",
      "Epoch [2/2], Step [791/938], Loss: 0.1106\n",
      "Epoch [2/2], Step [792/938], Loss: 0.0457\n",
      "Epoch [2/2], Step [793/938], Loss: 0.0966\n",
      "Epoch [2/2], Step [794/938], Loss: 0.0592\n",
      "Epoch [2/2], Step [795/938], Loss: 0.0565\n",
      "Epoch [2/2], Step [796/938], Loss: 0.0630\n",
      "Epoch [2/2], Step [797/938], Loss: 0.0580\n",
      "Epoch [2/2], Step [798/938], Loss: 0.1589\n",
      "Epoch [2/2], Step [799/938], Loss: 0.1055\n",
      "Epoch [2/2], Step [800/938], Loss: 0.0788\n",
      "Epoch [2/2], Step [801/938], Loss: 0.0388\n",
      "Epoch [2/2], Step [802/938], Loss: 0.0097\n",
      "Epoch [2/2], Step [803/938], Loss: 0.1067\n",
      "Epoch [2/2], Step [804/938], Loss: 0.1573\n",
      "Epoch [2/2], Step [805/938], Loss: 0.0621\n",
      "Epoch [2/2], Step [806/938], Loss: 0.0279\n",
      "Epoch [2/2], Step [807/938], Loss: 0.1206\n",
      "Epoch [2/2], Step [808/938], Loss: 0.0353\n",
      "Epoch [2/2], Step [809/938], Loss: 0.0715\n",
      "Epoch [2/2], Step [810/938], Loss: 0.1104\n",
      "Epoch [2/2], Step [811/938], Loss: 0.3625\n",
      "Epoch [2/2], Step [812/938], Loss: 0.1221\n",
      "Epoch [2/2], Step [813/938], Loss: 0.3023\n",
      "Epoch [2/2], Step [814/938], Loss: 0.1180\n",
      "Epoch [2/2], Step [815/938], Loss: 0.0868\n",
      "Epoch [2/2], Step [816/938], Loss: 0.1581\n",
      "Epoch [2/2], Step [817/938], Loss: 0.1609\n",
      "Epoch [2/2], Step [818/938], Loss: 0.2699\n",
      "Epoch [2/2], Step [819/938], Loss: 0.1152\n",
      "Epoch [2/2], Step [820/938], Loss: 0.1357\n",
      "Epoch [2/2], Step [821/938], Loss: 0.3962\n",
      "Epoch [2/2], Step [822/938], Loss: 0.1112\n",
      "Epoch [2/2], Step [823/938], Loss: 0.1101\n",
      "Epoch [2/2], Step [824/938], Loss: 0.1085\n",
      "Epoch [2/2], Step [825/938], Loss: 0.2198\n",
      "Epoch [2/2], Step [826/938], Loss: 0.1858\n",
      "Epoch [2/2], Step [827/938], Loss: 0.2159\n",
      "Epoch [2/2], Step [828/938], Loss: 0.1671\n",
      "Epoch [2/2], Step [829/938], Loss: 0.0411\n",
      "Epoch [2/2], Step [830/938], Loss: 0.1604\n",
      "Epoch [2/2], Step [831/938], Loss: 0.1594\n",
      "Epoch [2/2], Step [832/938], Loss: 0.3235\n",
      "Epoch [2/2], Step [833/938], Loss: 0.0833\n",
      "Epoch [2/2], Step [834/938], Loss: 0.0251\n",
      "Epoch [2/2], Step [835/938], Loss: 0.1378\n",
      "Epoch [2/2], Step [836/938], Loss: 0.0715\n",
      "Epoch [2/2], Step [837/938], Loss: 0.2163\n",
      "Epoch [2/2], Step [838/938], Loss: 0.0504\n",
      "Epoch [2/2], Step [839/938], Loss: 0.1846\n",
      "Epoch [2/2], Step [840/938], Loss: 0.0276\n",
      "Epoch [2/2], Step [841/938], Loss: 0.0752\n",
      "Epoch [2/2], Step [842/938], Loss: 0.0343\n",
      "Epoch [2/2], Step [843/938], Loss: 0.0730\n",
      "Epoch [2/2], Step [844/938], Loss: 0.0964\n",
      "Epoch [2/2], Step [845/938], Loss: 0.1227\n",
      "Epoch [2/2], Step [846/938], Loss: 0.2956\n",
      "Epoch [2/2], Step [847/938], Loss: 0.0475\n",
      "Epoch [2/2], Step [848/938], Loss: 0.3327\n",
      "Epoch [2/2], Step [849/938], Loss: 0.1464\n",
      "Epoch [2/2], Step [850/938], Loss: 0.1272\n",
      "Epoch [2/2], Step [851/938], Loss: 0.0684\n",
      "Epoch [2/2], Step [852/938], Loss: 0.0425\n",
      "Epoch [2/2], Step [853/938], Loss: 0.1472\n",
      "Epoch [2/2], Step [854/938], Loss: 0.2714\n",
      "Epoch [2/2], Step [855/938], Loss: 0.0796\n",
      "Epoch [2/2], Step [856/938], Loss: 0.0354\n",
      "Epoch [2/2], Step [857/938], Loss: 0.2196\n",
      "Epoch [2/2], Step [858/938], Loss: 0.1310\n",
      "Epoch [2/2], Step [859/938], Loss: 0.1010\n",
      "Epoch [2/2], Step [860/938], Loss: 0.0302\n",
      "Epoch [2/2], Step [861/938], Loss: 0.0643\n",
      "Epoch [2/2], Step [862/938], Loss: 0.1100\n",
      "Epoch [2/2], Step [863/938], Loss: 0.1152\n",
      "Epoch [2/2], Step [864/938], Loss: 0.1312\n",
      "Epoch [2/2], Step [865/938], Loss: 0.1732\n",
      "Epoch [2/2], Step [866/938], Loss: 0.0540\n",
      "Epoch [2/2], Step [867/938], Loss: 0.0611\n",
      "Epoch [2/2], Step [868/938], Loss: 0.0531\n",
      "Epoch [2/2], Step [869/938], Loss: 0.2803\n",
      "Epoch [2/2], Step [870/938], Loss: 0.0575\n",
      "Epoch [2/2], Step [871/938], Loss: 0.2233\n",
      "Epoch [2/2], Step [872/938], Loss: 0.0399\n",
      "Epoch [2/2], Step [873/938], Loss: 0.0680\n",
      "Epoch [2/2], Step [874/938], Loss: 0.1232\n",
      "Epoch [2/2], Step [875/938], Loss: 0.1391\n",
      "Epoch [2/2], Step [876/938], Loss: 0.1051\n",
      "Epoch [2/2], Step [877/938], Loss: 0.0134\n",
      "Epoch [2/2], Step [878/938], Loss: 0.0223\n",
      "Epoch [2/2], Step [879/938], Loss: 0.2883\n",
      "Epoch [2/2], Step [880/938], Loss: 0.0568\n",
      "Epoch [2/2], Step [881/938], Loss: 0.0546\n",
      "Epoch [2/2], Step [882/938], Loss: 0.0807\n",
      "Epoch [2/2], Step [883/938], Loss: 0.1128\n",
      "Epoch [2/2], Step [884/938], Loss: 0.0755\n",
      "Epoch [2/2], Step [885/938], Loss: 0.1468\n",
      "Epoch [2/2], Step [886/938], Loss: 0.1118\n",
      "Epoch [2/2], Step [887/938], Loss: 0.2362\n",
      "Epoch [2/2], Step [888/938], Loss: 0.0185\n",
      "Epoch [2/2], Step [889/938], Loss: 0.0297\n",
      "Epoch [2/2], Step [890/938], Loss: 0.1977\n",
      "Epoch [2/2], Step [891/938], Loss: 0.1115\n",
      "Epoch [2/2], Step [892/938], Loss: 0.0562\n",
      "Epoch [2/2], Step [893/938], Loss: 0.0922\n",
      "Epoch [2/2], Step [894/938], Loss: 0.0541\n",
      "Epoch [2/2], Step [895/938], Loss: 0.0555\n",
      "Epoch [2/2], Step [896/938], Loss: 0.0876\n",
      "Epoch [2/2], Step [897/938], Loss: 0.0956\n",
      "Epoch [2/2], Step [898/938], Loss: 0.0339\n",
      "Epoch [2/2], Step [899/938], Loss: 0.0933\n",
      "Epoch [2/2], Step [900/938], Loss: 0.0334\n",
      "Epoch [2/2], Step [901/938], Loss: 0.2493\n",
      "Epoch [2/2], Step [902/938], Loss: 0.1507\n",
      "Epoch [2/2], Step [903/938], Loss: 0.0362\n",
      "Epoch [2/2], Step [904/938], Loss: 0.1070\n",
      "Epoch [2/2], Step [905/938], Loss: 0.0197\n",
      "Epoch [2/2], Step [906/938], Loss: 0.1161\n",
      "Epoch [2/2], Step [907/938], Loss: 0.1686\n",
      "Epoch [2/2], Step [908/938], Loss: 0.1274\n",
      "Epoch [2/2], Step [909/938], Loss: 0.0585\n",
      "Epoch [2/2], Step [910/938], Loss: 0.0884\n",
      "Epoch [2/2], Step [911/938], Loss: 0.1290\n",
      "Epoch [2/2], Step [912/938], Loss: 0.2004\n",
      "Epoch [2/2], Step [913/938], Loss: 0.0527\n",
      "Epoch [2/2], Step [914/938], Loss: 0.2161\n",
      "Epoch [2/2], Step [915/938], Loss: 0.1470\n",
      "Epoch [2/2], Step [916/938], Loss: 0.0900\n",
      "Epoch [2/2], Step [917/938], Loss: 0.1587\n",
      "Epoch [2/2], Step [918/938], Loss: 0.4030\n",
      "Epoch [2/2], Step [919/938], Loss: 0.1139\n",
      "Epoch [2/2], Step [920/938], Loss: 0.1322\n",
      "Epoch [2/2], Step [921/938], Loss: 0.1805\n",
      "Epoch [2/2], Step [922/938], Loss: 0.0336\n",
      "Epoch [2/2], Step [923/938], Loss: 0.0351\n",
      "Epoch [2/2], Step [924/938], Loss: 0.0209\n",
      "Epoch [2/2], Step [925/938], Loss: 0.0225\n",
      "Epoch [2/2], Step [926/938], Loss: 0.0443\n",
      "Epoch [2/2], Step [927/938], Loss: 0.0822\n",
      "Epoch [2/2], Step [928/938], Loss: 0.2144\n",
      "Epoch [2/2], Step [929/938], Loss: 0.1207\n",
      "Epoch [2/2], Step [930/938], Loss: 0.0365\n",
      "Epoch [2/2], Step [931/938], Loss: 0.0663\n",
      "Epoch [2/2], Step [932/938], Loss: 0.1338\n",
      "Epoch [2/2], Step [933/938], Loss: 0.1059\n",
      "Epoch [2/2], Step [934/938], Loss: 0.0502\n",
      "Epoch [2/2], Step [935/938], Loss: 0.0484\n",
      "Epoch [2/2], Step [936/938], Loss: 0.0908\n",
      "Epoch [2/2], Step [937/938], Loss: 0.1657\n",
      "Epoch [2/2], Step [938/938], Loss: 0.3518\n"
     ]
    }
   ],
   "source": [
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx ,(data, target) in enumerate(train_loader):\n",
    "        #Get data to cuda\n",
    "        data = data.to(device=device).squeeze(1)\n",
    "        target = target.to(device=device)\n",
    "        \n",
    "        #Get to correct shape\n",
    "        #data = data.reshape(data.shape[0], -1)\n",
    "\n",
    "        #forward\n",
    "        scores = model(data)\n",
    "        loss = criterion(scores, target)\n",
    "\n",
    "        #Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        #Gradient Descent or Adam Step\n",
    "        optimizer.step()\n",
    "        print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                       .format(epoch+1, num_epochs, batch_idx+1, total_step, loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55bec3cd",
   "metadata": {},
   "source": [
    "Check Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c5e0b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(loader, model):\n",
    "    if loader.dataset.train:\n",
    "        print(\"Checking accuracy on training data\")\n",
    "    else:\n",
    "        print(\"Checking accuracy on test data\")\n",
    "    num_correct = 0\n",
    "    num_samples=0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x,y in loader:\n",
    "            x = x.to(device=device).squeeze(1)\n",
    "            y = y.to(device=device)\n",
    "            #x = x.reshape(x.shape[0], -1)\n",
    "\n",
    "            scores = model(x)\n",
    "            _,pred = scores.max(1)\n",
    "            num_correct += (pred == y).sum()\n",
    "            num_samples += pred.size(0)\n",
    "\n",
    "        print(f'Got {num_correct}/{num_samples} with accuracy {float(num_correct)/float(num_samples)*100:.2f}')\n",
    "    \n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d8199db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking accuracy on training data\n",
      "Got 58089/60000 with accuracy 96.81\n",
      "Checking accuracy on test data\n",
      "Got 9650/10000 with accuracy 96.50\n"
     ]
    }
   ],
   "source": [
    "evaluate(train_loader, model)\n",
    "evaluate(test_loader, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
